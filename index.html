<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>Iris.Alex</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  
    <link rel="shortcut icon" href="/blog/favicon.png">
  
  
<link rel="stylesheet" href="/blog/css/style.css">

</head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/blog/" id="logo">Iris.Alex</a>
      </h1>
      
        <h2 id="subtitle-wrap">
          <a href="/blog/" id="subtitle">alex</a>
        </h2>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/blog/">Home</a>
        
          <a class="main-nav-link" href="/blog/archives">Archives</a>
        
          <a class="main-nav-link" href="/blog/categories">Categories</a>
        
          <a class="main-nav-link" href="/blog/tags">Tags</a>
        
          <a class="main-nav-link" href="/blog/about">About</a>
        
      </nav>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main">
  
    <article id="post-Ai" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/blog/2025/03/13/Ai/" class="article-date">
  <time class="dt-published" datetime="2025-03-13T09:31:00.000Z" itemprop="datePublished">2025-03-13</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/blog/2025/03/13/Ai/">大模型</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="大模型到底是什么？"><a href="#大模型到底是什么？" class="headerlink" title="大模型到底是什么？"></a>大模型到底是什么？</h1><hr>
<h4 id="什么是大模型？"><a href="#什么是大模型？" class="headerlink" title="什么是大模型？"></a>什么是大模型？</h4><p>&emsp;&emsp;大模型（Large Model）通常指的是参数规模庞大的人工智能模型，尤其是基于深度学习的神经网络模型。这些模型通常具备强大的自然语言处理（NLP）、计算机视觉、语音识别等能力，能够执行复杂的推理、生成和决策任务。<br><br>&emsp;&emsp; 简单来说，大模型就像一个超级聪明的大脑，通过“学习”大量的数据（比如书籍、文章、网页内容等），掌握了语言、逻辑甚至某种程度的推理能力。像(chatgpt),（deepseek）就是一个大模型的例子，目的是帮助用户理解世界、回答问题。<br><br>&emsp;&emsp;从技术角度看，大模型通常基于 Transformer 架构（一种深度学习框架），通过预训练（Pre-training）和微调（Fine-tuning）的方式，让模型在通用知识的基础上适应特定任务。比如，被训练来对话，能尽量自然、准确地回应用户提出的问题。</p>
<h3 id="大模型的特点"><a href="#大模型的特点" class="headerlink" title="大模型的特点"></a>大模型的特点</h3><ol>
<li><p><strong>超大参数量：</strong>通常包含数十亿甚至上万亿个参数，如 OpenAI 的 GPT-4、Google 的 Gemini、Meta 的 LLaMA ，xAI的 Grok 3等。</p>
</li>
<li><p><strong>大规模数据训练：</strong> 使用海量的文本、代码、图片、语音等数据进行训练，使其具备广泛的知识和推理能力。</p>
</li>
<li><p><strong>通用性强：</strong> 能够处理多种任务，包括文本生成、翻译、代码编写、图片生成、自动问答等。</p>
</li>
<li><p><strong>多模态能力：</strong> 一些大模型不仅支持文本，还可以处理图像、音频、视频等多种输入（如 GPT-4V、Gemini 1.5）。</p>
</li>
<li><p><strong>强大的推理能力：</strong>能够理解复杂语境、进行逻辑推理，甚至在一定程度上进行创造性思考。</p>
</li>
</ol>
<hr>
<h4 id="技术角度"><a href="#技术角度" class="headerlink" title="技术角度"></a>技术角度</h4><p><strong>1. 架构基础：Transformer</strong> <br><br>&emsp;&emsp;大模型通常基于 Transformer 架构，这是 2017 年谷歌提出的一个深度学习模型框架，特别适合处理序列数据（比如句子）。Transformer 的核心是“自注意力机制”（Self-Attention），简单说就是模型能自己判断一句话里哪些词更重要。比如在“我喜欢吃苹果”里，“喜欢”和“苹果”可能比“吃”更关键，模型会自动给它们分配更多“关注”。<br><br><strong>编码器和解码器：</strong>Transformer 分两部分。编码器把输入（比如你的问题）转化成一种数学表示（向量）；解码器把这个表示生成输出。</p>
<p> <strong>多层堆叠：</strong>大模型会把多个 Transformer 层叠起来（几十层甚至上百层），每层都更深入地理解和加工信息。</p>
<p><strong>2. 参数规模</strong><br><br>大模型之所以“大”，是因为它有巨量的参数。参数可以理解为模型大脑里的“神经连接”，用来记住和处理信息。比如：</p>
<ul>
<li><p><strong>GPT-3 有 1750 亿个参数。</strong></p>
</li>
<li><p><strong>更新的模型可能参数更多。</strong></p>
</li>
<li><p><strong>这些参数通过训练调整，让模型能“记住”语言规律、事实和逻辑。</strong></p>
</li>
</ul>
<p><strong>3. 训练过程</strong></p>
<p>训练大模型是个超级复杂的工程，分两步：</p>
<ul>
<li><p><strong>预训练（Pre-training）：</strong>模型先在一大堆无标注的数据上（比如整个互联网的文本）学习语言的基本规律。比如，它会预测一句话的下半句是什么（“我喜欢吃…” → “苹果”），或者填空。这种方式让模型掌握通用知识。</p>
</li>
<li><p><strong>微调（Fine-tuning）：</strong>在特定任务上（比如对话、翻译）用更小的数据集调整模型，让它更擅长某个领域。我就被微调过，能更好地回答问题和聊天。训练需要海量计算资源，比如成千上万块 GPU，跑几天甚至几个月。</p>
</li>
</ul>
<p><strong>4. 数据和计算</strong></p>
<ul>
<li><p><strong>数据：</strong>大模型需要吞噬海量文本数据（想想维基百科、新闻、论坛帖子等等），可能有几百 TB。</p>
</li>
<li><p><strong>计算力：</strong> 训练靠超级计算机集群，成本动辄几百万美元。</p>
</li>
</ul>
<p><strong>5. 推理（Inference）</strong><br>训练完后，模型进入“推理”阶段，也就是实际使用时。比如你问问题：</p>
<ul>
<li><p><strong>大模型把你的输入转成数字（向量化）。</strong></p>
</li>
<li><p><strong>通过多层 Transformer 处理，生成回答的数字表示。</strong></p>
</li>
<li><p><strong>再把数字转回结果输出。</strong></p>
</li>
</ul>
<p>这过程很快，但背后是亿万次计算。</p>
<hr>
<h1 id="Transformer-的核心技术"><a href="#Transformer-的核心技术" class="headerlink" title="Transformer 的核心技术"></a>Transformer 的核心技术</h1><h4 id="Transformer-核心技术自注意力（Self-Attention）怎么算？"><a href="#Transformer-核心技术自注意力（Self-Attention）怎么算？" class="headerlink" title="Transformer 核心技术自注意力（Self-Attention）怎么算？"></a>Transformer 核心技术自注意力（Self-Attention）怎么算？</h4><p>&emsp;&emsp;自注意力是 Transformer 的核心，它让模型在处理一句话时，知道每个词和其他词的关系，以及哪些词更重要。计算过程可以拆成几个步骤：</p>
<p><strong>1. 输入向量化</strong><br>&emsp;&emsp; 假设输入是“我喜欢吃苹果”。每个词会被转化成一个数字向量（比如 512 维或 768 维，具体看模型设计）。这些向量是模型通过预训练学会的，代表词的意思。</p>
<ul>
<li><p>“我” → [0.1, -0.3, 0.5, …]（一个向量）</p>
</li>
<li><p>“喜欢” → [0.2, 0.4, -0.1, …]</p>
</li>
<li><p>以此类推。</p>
</li>
</ul>
<p><strong>2. 生成 Q、K、V 向量</strong><br>&emsp;&emsp;自注意力需要三种向量：<strong>查询（Query, Q）</strong>、<strong>键（Key, K）</strong>和<strong>值（Value, V）</strong>。这三种向量是从输入向量通过线性变换（矩阵乘法）算出来的。 <br><br><strong>用三个不同的权重矩阵 𝑊𝑄,𝑊𝐾,𝑊𝑉：</strong></p>
<ul>
<li><p>Q&#x3D;输入向量  x wQ</p>
</li>
<li><p>k&#x3D;输入向量  x wk</p>
</li>
<li><p>v&#x3D;输入向量  x wv</p>
</li>
</ul>
<p>比如“我”的向量乘以 Wq 得到它的 Q 向量，表示“我”在问其他词的关系。</p>
<p>这些矩阵是模型训练时学到的。</p>
<p><strong>3. 计算注意力分数</strong><br>接下来，算每个词对其他词的“关注度”:</p>
<ul>
<li><p>用 Q 和 K 做点积（Dot Product），得到一个分数。比如“我”的 Q 和“喜欢”的 K 点积，得出“我”和“喜欢”的关系强度。</p>
</li>
<li><p>对所有词两两计算，得到一个分数矩阵：</p>
</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">     我   喜欢  吃   苹果</span><br><span class="line">我    0.8  0.3  0.1  0.6</span><br><span class="line">喜欢  0.3  0.9  0.2  0.4</span><br><span class="line">吃    0.1  0.2  0.7  0.3</span><br><span class="line">苹果  0.6  0.4  0.3  0.8</span><br></pre></td></tr></table></figure>

<p><strong>4. 归一化（Softmax）</strong><br>这些分数会通过 Softmax 函数归一化，变成概率分布。</p>
<p>比如“我”这行的分数 [0.8, 0.3, 0.1, 0.6] 可能变成 [0.4, 0.15, 0.05, 0.3]，总和为 1。</p>
<p><strong>5. 加权求和</strong><br>用这些概率去加权 V 向量，得到新的表示：</p>
<p>“我”的新向量 &#x3D; 0.4 × “我”的 V + 0.15 × “喜欢”的 V + 0.05 × “吃”的 V + 0.3 × “苹果”的 V。</p>
<p>这样，“我”的新表示融合了整句话的信息，但更关注“喜欢”和“苹果”。</p>
<p><strong>输出</strong> <br><br>经过自注意力，每词的向量都更新了，带着上下文信息，然后传到下一层 Transformer 处理。</p>
<hr>
<h4 id="从一层传到下一层的处理过程"><a href="#从一层传到下一层的处理过程" class="headerlink" title="从一层传到下一层的处理过程 "></a>从一层传到下一层的处理过程 <br></h4><h5 id="每一层具体做什么？"><a href="#每一层具体做什么？" class="headerlink" title="每一层具体做什么？"></a>每一层具体做什么？</h5><p>每一层的处理本质相同，但作用逐渐深化：</p>
<ul>
<li><p><strong>第一层：</strong>可能关注词与词的基本关系，比如“我”和“喜欢”绑定，“吃”和“苹果”关联。</p>
</li>
<li><p><strong>第二层：</strong>基于第一层的结果，可能发现更高阶的模式，比如“我喜欢”是主谓结构，“吃苹果”是动作对象。</p>
</li>
<li><p><strong>更深层：</strong>可能理解整个句子的意图（表达喜好），甚至推理出隐藏信息（比如“苹果”是食物）。</p>
</li>
</ul>
<h4 id="技术上角度"><a href="#技术上角度" class="headerlink" title="技术上角度"></a>技术上角度</h4><p><strong>自注意力：</strong> 每层重新计算词之间的关系，但因为输入更丰富（带着上一层的上下文），结果更精准。</p>
<p><strong>前馈网络：</strong> 对每个词的表示做非线性变换，增强表达能力。</p>
<p><strong>残差和归一化：</strong>保证信息不丢失，梯度稳定。</p>
<hr>
<h4 id="实际例子："><a href="#实际例子：" class="headerlink" title="实际例子："></a>实际例子：</h4><p>假设你问“Transformer 是什么”：</p>
<ul>
<li><p><strong>第一层：</strong>理解“Transformer”和“是什么”的基本关系。</p>
</li>
<li><p><strong>第二层：</strong>结合上下文，知道你想要技术解释。</p>
</li>
<li><p><strong>更深层：</strong>组织回答的逻辑，把技术细节串起来。 每一层输出传到下一层，逐步完善我的回答。</p>
</li>
</ul>
<hr>
<h4 id="技术细节和难点"><a href="#技术细节和难点" class="headerlink" title="技术细节和难点 "></a>技术细节和难点 <br></h4><ul>
<li><p><strong>参数独立：</strong> 每层有自己的权重，训练时分别优化。</p>
</li>
<li><p><strong>计算成本：</strong> 自注意力复杂度是 𝑂(𝑛2⋅𝑑)O(n2⋅d)（𝑛n 是序列长度，𝑑d 是维度），长序列很耗资源。</p>
</li>
<li><p><strong>梯度传递：</strong>残差和归一化保证深层网络不崩溃。</p>
</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://irisalex.github.io/blog/2025/03/13/Ai/" data-id="cm879ko9000033zjjaigr0rzm" data-title="大模型" class="article-share-link">Share</a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-8种流行的网络模型" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/blog/2023/12/04/8%E7%A7%8D%E6%B5%81%E8%A1%8C%E7%9A%84%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B/" class="article-date">
  <time class="dt-published" datetime="2023-12-04T02:32:30.000Z" itemprop="datePublished">2023-12-04</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/blog/2023/12/04/8%E7%A7%8D%E6%B5%81%E8%A1%8C%E7%9A%84%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B/">8种流行的网络模型</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h3 id="最流行的-8-种网路协议"><a href="#最流行的-8-种网路协议" class="headerlink" title="最流行的 8 种网路协议"></a>最流行的 8 种网路协议</h3><p>目前网络上最流行的就是以下 8种网路模型。下面就一一为大家详解介绍一下这8种模型的用法与意义。</p>
 <img src="/blog/images/image.png" width = "100%" height = "100%" align=center />
 
<p>让我们开始吧</p>
 <img src="/blog/images/pasted-59.png" width = "100%" height = "100%" align=center />

<h4 id="http-协议"><a href="#http-协议" class="headerlink" title="http 协议"></a>http 协议</h4> <img src="/blog/images/pasted-60.png" width = "100%" height = "100%" align=center />

<p><strong>首先是http , 它是 web 的支柱， http 使用请求 - 响应的模型。</strong> </p>
 <img src="/blog/images/pasted-67.png" width = "100%" height = "100%" align=center />

<p><strong>客户端向服务器发送请求，包括 （图片，页面，api 数据等资源）<br>服务器返回带有 code 码的资源信息</strong></p>
 <img src="/blog/images/pasted-68.png" width = "100%" height = "100%" align=center />

<p>信息包括 </p>
<p> <img src="/blog/images/pasted-70.png" width = "100%" height = "100%" align=center />等</p>
<p><strong>成功以后返回 200 码。</strong><br> <img src="/blog/images/pasted-73.png" width = "100%" height = "100%" align=center /></p>
<p> <strong>http 有 4种模型 分别是</strong><br> <img src="/blog/images/pasted-75.png" width = "100%" height = "100%" align=center /></p>
<ul>
<li><strong>POST</strong><br>  <strong>一般在创建资源的时候使用</strong></li>
<li><strong>GET</strong><br>  <strong>在获取资源的时候使用</strong></li>
<li><strong>PUT</strong><br>  <strong>在更新资源的时候使用</strong></li>
<li><strong>DELETE</strong><br>  <strong>在删除资源的时候使用</strong></li>
</ul>
<h4 id="https协议"><a href="#https协议" class="headerlink" title="https协议"></a>https协议</h4><p><strong>https 是基于 http + tls 传输安全性协议 添加的加密协议。</strong><br> <img src="/blog/images/pasted-76.png" width = "100%" height = "100%" align=center /></p>
<p>首先 http 通过 tcp 建立链接，然后将彼此的公钥发送给对方，客户端通过对方传过来的公钥加密session key 然后发送到服务器，服务器解密 session key 然后通过 session key 加密的数据来进行数据传输</p>
 <img src="/blog/images/pasted-77.png" width = "100%" height = "100%" align=center />

<p>https 通过 加密链接保证了数据的安全性 </p>
 <img src="/blog/images/pasted-80.png" width = "100%" height = "100%" align=center />
![upload successful](/images/pasted-80.png) 

<p>这种加密方式在互联网上可以保证数据数据的机密性。可以防止被黑客在中途截取数据<br>即使黑客拿到数据以后因为不知道加密的 session key 是什么所以即使拿到数据也是一堆 乱码。毫无意义。</p>
 <img src="/blog/images/pasted-81.png" width = "100%" height = "100%" align=center />

<h4 id="http3-协议"><a href="#http3-协议" class="headerlink" title="http3 协议"></a>http3 协议</h4><p>http3 旨在 提高传输的速度与安全性。 修复以前版本中一些棘手的问题。<br>http 3 使用了 quic   基于 udp 协议而不是 tcp 协议来链接。 quic 可以最大限度的减少智能手机上网络切换的延迟问题。<br> <img src="/blog/images/pasted-82.png" width = "100%" height = "100%" align=center /></p>
<p>quic 也消除了对头柱塞，即一个数据包丢失导致后边的数据包的流停滞问题。</p>
 <img src="/blog/images/pasted-83.png" width = "100%" height = "100%" align=center />
 
<h4 id="web-socket-协议"><a href="#web-socket-协议" class="headerlink" title="web socket 协议"></a>web socket 协议</h4><p>在单个 tcp 中提供全双工的通信。<br> <img src="/blog/images/pasted-84.png" width = "100%" height = "100%" align=center /></p>
<p>web socket 可以实现无缝实时的协作与实时数据流 </p>
 <img src="/blog/images/pasted-85.png" width = "100%" height = "100%" align=center />
 
<p>初始的时候 ws 会重用 tcp 链接。 然后消息可以在最少的桢在俩个方向上流动。</p>
 <img src="/blog/images/pasted-86.png" width = "100%" height = "100%" align=center />

<p>ws 支持以非常低的开销立即发送小数据消息。非常适合 聊天，游戏，或实时更新的数据。 </p>
<h4 id="tcp-udp-协议"><a href="#tcp-udp-协议" class="headerlink" title="tcp &#x2F; udp 协议"></a>tcp &#x2F; udp 协议</h4><p>tcp &#x2F; upd   是我们经常用的网络层传输协议。</p>
 <img src="/blog/images/pasted-87.png" width = "100%" height = "100%" align=center />
      
    </div>
    <footer class="article-footer">
      <a data-url="https://irisalex.github.io/blog/2023/12/04/8%E7%A7%8D%E6%B5%81%E8%A1%8C%E7%9A%84%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B/" data-id="cm879ko8y00013zjj8c5xehon" data-title="8种流行的网络模型" class="article-share-link">Share</a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-NoSQL-的应用" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/blog/2023/12/01/NoSQL-%E7%9A%84%E5%BA%94%E7%94%A8/" class="article-date">
  <time class="dt-published" datetime="2023-12-01T03:10:06.000Z" itemprop="datePublished">2023-12-01</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/blog/2023/12/01/NoSQL-%E7%9A%84%E5%BA%94%E7%94%A8/">NoSQL 的应用</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="NoSQL：在高并发场景下，数据库和NoSQL如何做到互补？"><a href="#NoSQL：在高并发场景下，数据库和NoSQL如何做到互补？" class="headerlink" title=" NoSQL：在高并发场景下，数据库和NoSQL如何做到互补？"></a><a href="#_11%E4%B8%A8nosql-%E5%9C%A8%E9%AB%98%E5%B9%B6%E5%8F%91%E5%9C%BA%E6%99%AF%E4%B8%8B-%E6%95%B0%E6%8D%AE%E5%BA%93%E5%92%8Cnosql%E5%A6%82%E4%BD%95%E5%81%9A%E5%88%B0%E4%BA%92%E8%A1%A5"></a> NoSQL：在高并发场景下，数据库和NoSQL如何做到互补？</h1><p>之前我带你了解了在你的垂直电商项目中，如何将传统的关系型数据库改造成分布式存储服务，以抵抗高并发和大流量的冲击。</p>
<p>对于存储服务来说，我们一般会从两个方面对它做改造：</p>
<ol>
<li><p>提升它的读写性能，尤其是读性能，因为我们面对的多是一些读多写少的产品。比方说，你离不开的微信朋友圈、微博和淘宝，都是查询 QPS 远远大于写入 QPS。</p>
</li>
<li><p>增强它在存储上的扩展能力，从而应对大数据量的存储需求。</p>
</li>
</ol>
<p>之前分享的读写分离和分库分表就是从这两方面出发，改造传统的关系型数据库的，但仍有一些问题无法解决。</p>
<p>比如，在微博项目中关系的数据量达到了千亿，那么即使分隔成 1024 个库表，每张表的数据量也达到了亿级别，并且关系的数据量还在以极快的速度增加，即使你分隔成再多的库表，数据量也会很快增加到瓶颈。这个问题用传统数据库很难根本解决，因为它在扩展性方面是很弱的，这时，就可以利用 NoSQL，因为它有着天生分布式的能力，能够提供优秀的读写性能，可以很好地补充传统关系型数据库的短板。那么它是如何做到的呢？</p>
<p>今天，我就还是以垂直电商系统为例，<strong>如何用 NoSQL 数据库和关系型数据库互补</strong> ，共同承担高并发和大流量的冲击。</p>
<p>首先，我们先来了解一下 NoSQL 数据库。</p>
<h2 id="NoSQL，No-SQL？"><a href="#NoSQL，No-SQL？" class="headerlink" title=" NoSQL，No SQL？"></a><a href="#nosql-no-sql"></a> NoSQL，No SQL？</h2><p>NoSQL 想必你很熟悉，它指的是不同于传统的关系型数据库的其他数据库系统的统称，它不使用 SQL 作为查询语言，提供优秀的横向扩展能力和读写性能，非常契合互联网项目高并发大数据的特点。所以一些大厂，比如小米、微博、陌陌都很倾向使用它来作为高并发大容量的数据存储服务。</p>
<p>NoSQL 数据库发展到现在，十几年间，出现了多种类型，我来给你举几个例子：</p>
<ul>
<li>Redis、LevelDB 这样的 <strong>KV 存储</strong>。这类存储相比于传统的数据库的优势是极高的读写性能，一般对性能有比较高的要求的场景会使用。</li>
<li>Hbase、Cassandra 这样的 <strong>列式存储数据库</strong>。这种数据库的特点是数据不像传统数据库以行为单位来存储，而是以列来存储，适用于一些离线数据统计的场景。</li>
<li>像 MongoDB、CouchDB 这样的文档型数据库。这种数据库的特点是 Schema Free（模式自由），数据表中的字段可以任意扩展，比如说电商系统中的商品有非常多的字段，并且不同品类的商品的字段也都不尽相同，使用关系型数据库就需要不断增加字段支持，而用文档型数据库就简单很多了。</li>
</ul>
<p>在 NoSQL 数据库刚刚被应用时，它被认为是可以替代关系型数据库的银弹，在我看来，也许因为以下几个方面的原因：</p>
<ul>
<li>弥补了传统数据库在性能方面的不足；</li>
<li>数据库变更方便，不需要更改原先的数据结构；</li>
<li>适合互联网项目常见的大数据量的场景；</li>
</ul>
<p>不过，这种看法是个误区，因为慢慢地我们发现在业务开发的场景下还是需要利用 SQL 语句的强大的查询功能以及传统数据库事务和灵活的索引等功能，NoSQL 只能作为一些场景的补充。</p>
<p>那么接下来，我就带你了解 <strong>NoSQL 数据库是如何做到与关系数据库互补的。</strong> 了解这部分内容，你可以在实际项目中更好地使用 NoSQL 数据库补充传统数据库的不足。</p>
<p>首先，我们来关注一下数据库的写入性能。</p>
<h2 id="使用-NoSQL-提升写入性能"><a href="#使用-NoSQL-提升写入性能" class="headerlink" title="# 使用 NoSQL 提升写入性能"></a><a href="#%E4%BD%BF%E7%94%A8-nosql-%E6%8F%90%E5%8D%87%E5%86%99%E5%85%A5%E6%80%A7%E8%83%BD">#</a> 使用 NoSQL 提升写入性能</h2><p>数据库系统大多使用的是传统的机械磁盘，对于机械磁盘的访问方式有两种：</p>
<ul>
<li><p>一种是随机 IO</p>
<p>随机 IO 就需要花费时间做昂贵的磁盘寻道，一般来说，它的读写效率要比顺序 IO 小两到三个数量级，所以我们想要提升写入的性能就要尽量减少随机 IO</p>
</li>
<li><p>另一种是顺序 IO</p>
</li>
</ul>
<p>以 MySQL 的 InnoDB 存储引擎来说，更新 binlog、redolog、undolog <strong>都是在做顺序 IO</strong>，而更新 datafile 和索引文件则是在做随机 IO，而为了减少随机 IO 的发生，关系数据库已经做了很多的优化，比如说写入时先写入内存，然后批量刷新到磁盘上，但是随机 IO 还是会发生。</p>
<p>索引在 InnoDB 引擎中是以 B+ 树（上一节课提到了 B+ 树，你可以回顾一下）方式来组织的，而 MySQL 主键是聚簇索引（一种索引类型，数据与索引数据放在一起），既然数据和索引数据放在一起，那么在数据插入或者更新的时候，我们需要找到要插入的位置，再把数据写到特定的位置上，这就产生了随机的 IO。而且一旦发生了页分裂，就不可避免会做数据的移动，也会极大地损耗写入性能。</p>
<p><strong>NoSQL 数据库是怎么解决这个问题的呢？</strong></p>
<p>它们有多种的解决方式，这里我给你讲一种最常见的方案，就是很多 NoSQL 数据库都在使用的 <strong>基于 LSM 树的存储引擎，</strong> 这种算法使用最多，所以在这里着重剖析一下。</p>
<p>LSM 树（Log-Structured Merge Tree）牺牲了一定的读性能来换取写入数据的高性能，Hbase、Cassandra、LevelDB 都是用这种算法作为存储的引擎。</p>
<p>它的思想很简单，数据首先会写入到一个叫做 MemTable 的内存结构中，在 MemTable 中数据是按照写入的 Key 来排序的。为了防止 MemTable 里面的数据因为机器掉电或者重启而丢失，一般会通过写 Write Ahead Log 的方式将数据备份在磁盘上。</p>
<p>MemTable 在累积到一定规模时，它会被刷新生成一个新的文件，我们把这个文件叫做 SSTable（Sorted String Table）。当 SSTable 达到一定数量时，我们会将这些 SSTable 合并，减少文件的数量，因为 SSTable 都是有序的，所以合并的速度也很快。</p>
<p>当从 LSM 树里面读数据时，我们首先从 MemTable 中查找数据，如果数据没有找到，再从 SSTable 中查找数据。因为存储的数据都是有序的，所以查找的效率是很高的，只是因为数据被拆分成多个 SSTable，所以读取的效率会低于 B+ 树索引。</p>
 <img src="/blog/images/pasted-54.png" width = "100%" height = "100%" align=center />

<p>和 LSM 树类似的算法有很多，比如说 TokuDB 使用的名为 Fractal tree 的索引结构，它们的核心思想就是将随机 IO 变成顺序的 IO，从而提升写入的性能。</p>
<p>在后面的缓存篇中，我也将给你着重介绍我们是如何使用 KV 型 NoSQL 存储来提升读性能的。所以你看，NoSQL 数据库补充关系型数据库的第一种方式就是提升读写性能。</p>
<h2 id="场景补充"><a href="#场景补充" class="headerlink" title="# 场景补充"></a><a href="#%E5%9C%BA%E6%99%AF%E8%A1%A5%E5%85%85">#</a> 场景补充</h2><p>除了可以提升性能之外，NoSQL 数据库还可以在某些场景下作为传统关系型数据库的补充，来看一个具体的例子。</p>
<p>假设某一天，CEO 找到你并且告诉你，他正在为你的垂直电商项目规划搜索的功能，需要支持按照商品的名称模糊搜索到对应的商品，希望你尽快调研出解决方案。</p>
<p>一开始，你认为这非常的简单，不就是在数据库里面执行一条类似：<code>select * from product where name like ‘%***%’</code> 的语句吗？可是在实际执行的过程中，却发现了问题。</p>
<p>你发现这类语句并不是都能使用到索引，只有后模糊匹配的语句才能使用索引。比如语句 <code>select * from product where name like ‘% 电冰箱’</code> 就没有使用到字段 <code>name</code> 上的索引，而 <code>select * from product where name like ‘索尼 %’</code> 就使用了 <code>name</code> 上的索引。而一旦没有使用索引就会扫描全表的数据，在性能上是无法接受的。</p>
<p>于是你在谷歌上搜索了一下解决方案，发现大家都在使用开源组件 Elasticsearch 来支持搜索的请求，它本身是基于“倒排索引”来实现的， <strong>那么什么是倒排索引呢？</strong></p>
<p>倒排索引是指将记录中的某些列做分词，然后形成的分词与记录 ID 之间的映射关系。比如说，你的垂直电商项目里面有以下记录：</p>
 <img src="/blog/images/pasted-55.png" width = "100%" height = "100%" align=center />

<p>那么，我们将商品名称做简单的分词，然后建立起分词和商品 ID 的对应关系，就像下面展示的这样：</p>
 <img src="/blog/images/pasted-56.png" width = "100%" height = "100%" align=center />

<p>这样，如果用户搜索电冰箱，就可以给他展示商品 ID 为 1 和 3 的两件商品了。</p>
<p>而 Elasticsearch 作为一种常见的 NoSQL 数据库， <strong>就以倒排索引作为核心技术原理，为你提供了分布式的全文搜索服务，这在传统的关系型数据库中使用 SQL 语句是很难实现的。</strong> 所以你看，NoSQL 可以在某些业务场景下代替传统数据库提供数据存储服务。</p>
<h2 id="提升扩展性"><a href="#提升扩展性" class="headerlink" title="# 提升扩展性"></a><a href="#%E6%8F%90%E5%8D%87%E6%89%A9%E5%B1%95%E6%80%A7">#</a> 提升扩展性</h2><p>另外，在扩展性方面，很多 NoSQL 数据库也有着先天的优势。还是以你的垂直电商系统为例，你已经为你的电商系统增加了评论系统，开始你的评估比较乐观，觉得电商系统的评论量级不会增长很快，所以就为它分了 8 个库，每个库拆分成 16 张表。</p>
<p>但是评论系统上线之后，存储量级增长的异常迅猛，你不得不将数据库拆分成更多的库表，而数据也要重新迁移到新的库表中，过程非常痛苦，而且数据迁移的过程也非常容易出错。</p>
<p>这时，你考虑是否可以考虑使用 NoSQL 数据库来彻底解决扩展性的问题，经过调研你发现它们在设计之初就考虑到了分布式和大数据存储的场景， <strong>比如像 MongoDB 就有三个扩展性方面的特性。</strong></p>
<ul>
<li><p>其一是 Replica，也叫做副本集，你可以理解为主从分离，也就是通过将数据拷贝成多份来保证当主挂掉后数据不会丢失。同时呢，Replica 还可以分担读请求。Replica 中有主节点来承担写请求，并且把对数据变动记录到 oplog 里（类似于 binlog）；从节点接收到 oplog 后就会修改自身的数据以保持和主节点的一致。一旦主节点挂掉，MongoDB 会从从节点中选取一个节点成为主节点，可以继续提供写数据服务。</p>
</li>
<li><p>其二是 Shard，也叫做分片，你可以理解为分库分表，即将数据按照某种规则拆分成多份，存储在不同的机器上。MongoDB 的 Sharding 特性一般需要三个角色来支持，一个是 Shard Server，它是实际存储数据的节点，是一个独立的 Mongod 进程；二是 Config Server，也是一组 Mongod 进程，主要存储一些元信息，比如说哪些分片存储了哪些数据等；最后是 Route Server，它不实际存储数据，仅仅作为路由使用，它从 Config Server 中获取元信息后，将请求路由到正确的 Shard Server 中。</p>
</li>
</ul>
 <img src="/blog/images/pasted-57.png" width = "100%" height = "100%" align=center />
    
<ul>
<li>其三是负载均衡，就是当 MongoDB 发现 Shard 之间数据分布不均匀，会启动 Balancer 进程对数据做重新的分配，最终让不同 Shard Server 的数据可以尽量的均衡。当我们的 Shard Server 存储空间不足需要扩容时，数据会自动被移动到新的 Shard Server 上，减少了数据迁移和验证的成本。</li>
</ul>
<p>你可以看到，NoSQL 数据库中内置的扩展性方面的特性可以让我们不再需要对数据库做分库分表和主从分离，也是对传统数据库一个良好的补充。</p>
<p>你可能会觉得，NoSQL 已经成熟到可以代替关系型数据库了，但是就目前来看，NoSQL 只能作为传统关系型数据库的补充而存在，弥补关系型数据库在性能、扩展性和某些场景下的不足，所以你在使用或者选择时要结合自身的场景灵活地运用。</p>
<h2 id="小结"><a href="#小结" class="headerlink" title="#小结"></a><a href="#%E8%AF%BE%E7%A8%8B%E5%B0%8F%E7%BB%93">#</a>小结</h2><p>今天我带你了解了 NoSQL 数据库在性能、扩展性上的优势，以及它的一些特殊功能特性，主要有以下几点：</p>
<ol>
<li><p>在性能方面，NoSQL 数据库使用一些算法将对磁盘的随机写转换成顺序写，提升了写的性能；</p>
</li>
<li><p>在某些场景下，比如全文搜索功能，关系型数据库并不能高效地支持，需要 NoSQL 数据库的支持；</p>
</li>
<li><p>在扩展性方面，NoSQL 数据库天生支持分布式，支持数据冗余和数据分片的特性。</p>
</li>
</ol>
<p>这些都让它成为传统关系型数据库的良好的补充，你需要了解的是， <strong>NoSQL 可供选型的种类很多，每一个组件都有各自的特点。你在做选型的时候需要对它的实现原理有比较深入的了解，最好在运维方面对它有一定的熟悉，这样在出现问题时才能及时找到解决方案。</strong> 否则，盲目跟从地上了一个新的 NoSQL 数据库，最终可能导致会出了故障无法解决，反而成为整体系统的拖累。</p>
<p>我在之前的项目中曾经使用 Elasticsearch 作为持久存储，支撑社区的 feed 流功能，初期开发的时候确实很爽，你可以针对 feed 中的任何字段做灵活高效地查询，业务功能迭代迅速，代码也简单易懂。可是到了后期流量上来之后，由于缺少对于 Elasticsearch 成熟的运维能力，造成故障频出，尤其到了高峰期就会出现节点不可用的问题，而由于业务上的巨大压力又无法分出人力和精力对 Elasticsearch 深入的学习和了解，最后不得不做大的改造切回熟悉的 MySQL。 <strong>所以，对于开源组件的使用，不能只停留在只会 hello world 的阶段，而应该对它有足够的运维上的把控能力。</strong></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://irisalex.github.io/blog/2023/12/01/NoSQL-%E7%9A%84%E5%BA%94%E7%94%A8/" data-id="cm879ko9100053zjj2b2k8lau" data-title="NoSQL 的应用" class="article-share-link">Share</a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-数据的唯一性" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/blog/2023/12/01/%E6%95%B0%E6%8D%AE%E7%9A%84%E5%94%AF%E4%B8%80%E6%80%A7/" class="article-date">
  <time class="dt-published" datetime="2023-12-01T03:02:33.000Z" itemprop="datePublished">2023-12-01</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/blog/2023/12/01/%E6%95%B0%E6%8D%AE%E7%9A%84%E5%94%AF%E4%B8%80%E6%80%A7/">数据的唯一性</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="如何保证分库分表后-ID-的全局唯一性？"><a href="#如何保证分库分表后-ID-的全局唯一性？" class="headerlink" title=" 如何保证分库分表后 ID 的全局唯一性？"></a><a href="#_10%E4%B8%A8%E5%8F%91%E5%8F%B7%E5%99%A8-%E5%A6%82%E4%BD%95%E4%BF%9D%E8%AF%81%E5%88%86%E5%BA%93%E5%88%86%E8%A1%A8%E5%90%8E-id-%E7%9A%84%E5%85%A8%E5%B1%80%E5%94%AF%E4%B8%80%E6%80%A7"></a> 如何保证分库分表后 ID 的全局唯一性？</h1><p>在之前的内容分享中我带你了解了分布式存储两个核心问题：数据冗余和数据分片，以及在传统关系型数据库中是如何解决的。</p>
<p>当我们面临高并发的查询数据请求时，可以使用主从读写分离的方式，部署多个从库分摊读压力；</p>
<p>当存储的数据量达到瓶颈时，我们可以将数据分片存储在多个节点上，降低单个存储节点的存储压力，此时我们的架构变成了下面这个样子：</p>
 <img src="/blog/images/pasted-50.png" width = "100%" height = "100%" align=center />

<p>你可以看到，我们通过分库分表和主从读写分离的方式解决了数据库的扩展性问题，但是在 09 讲我也提到过，数据库在分库分表之后，我们在使用数据库时存在的许多限制，比方说查询的时候必须带着分区键；一些聚合类的查询（像是 <code>count()</code>）性能较差，需要考虑使用计数器等其它的解决方案，其实分库分表还有一个问题我在 09 中没有提到，就是 <strong>主键的全局唯一性的问题</strong> 。本节课，我将带你一起来了解，在分库分表后如何生成全局唯一的数据库主键。</p>
<p>不过，在探究这个问题之前，你需要对「使用什么字段作为主键」这个问题有所了解，这样才能为我们后续探究如何生成全局唯一的主键做好铺垫。</p>
<h2 id="数据库的主键要如何选择？"><a href="#数据库的主键要如何选择？" class="headerlink" title=" 数据库的主键要如何选择？"></a><a href="#%E6%95%B0%E6%8D%AE%E5%BA%93%E7%9A%84%E4%B8%BB%E9%94%AE%E8%A6%81%E5%A6%82%E4%BD%95%E9%80%89%E6%8B%A9"></a> 数据库的主键要如何选择？</h2><p>数据库中的每一条记录都需要有一个唯一的标识，依据数据库的第二范式，数据库中每一个表中都需要有一个唯一的主键，其他数据元素和主键一一对应。</p>
<p><strong>那么关于主键的选择就成为一个关键点了，</strong> 一般来讲，你有两种选择方式：</p>
<ol>
<li><p>使用业务字段作为主键，比如说对于用户表来说，可以使用手机号，email 或者身份证号作为主键。</p>
</li>
<li><p>使用生成的唯一 ID 作为主键。</p>
</li>
</ol>
<p>不过对于大部分场景来说，第一种选择并不适用，比如像评论表你就很难找到一个业务字段作为主键，因为在评论表中，你很难找到一个字段唯一标识一条评论。而对于用户表来说，我们需要考虑的是作为主键的业务字段是否能够唯一标识一个人，一个人可以有多个 email 和手机号，一旦出现变更 email 或者手机号的情况，就需要变更所有引用的外键信息，所以使用 email 或者手机作为主键是不合适的。</p>
<p>身份证号码确实是用户的唯一标识，但是由于它的隐私属性，并不是一个用户系统的必须属性，你想想，你的系统如果没有要求做实名认证，那么肯定不会要求用户填写身份证号码的。并且已有的身份证号码是会变更的，比如在 1999 年时身份证号码就从 15 位变更为 18 位，但是主键一旦变更，以这个主键为外键的表也都要随之变更，这个工作量是巨大的。</p>
<p><strong>因此，我更倾向于使用生成的 ID 作为数据库的主键。</strong> 不单单是因为它的唯一性，更是因为一旦生成就不会变更，可以随意引用。</p>
<p>在单库单表的场景下，我们可以使用数据库的自增字段作为 ID，因为这样最简单，对于开发人员来说也是透明的。但是当数据库分库分表后，使用自增字段就无法保证 ID 的全局唯一性了。</p>
<p>想象一下，当我们分库分表之后，同一个逻辑表的数据被分布到多个库中，这时如果使用数据库自增字段作为主键，那么只能保证在这个库中是唯一的，无法保证全局的唯一性。那么假如你来设计用户系统的时候，使用自增 ID 作为用户 ID，就可能出现两个用户有两个相同的 ID，这是不可接受的，那么你要怎么做呢？我建议你搭建发号器服务来生成全局唯一的 ID。</p>
<h2 id="基于-Snowflake-算法搭建发号器"><a href="#基于-Snowflake-算法搭建发号器" class="headerlink" title=" 基于 Snowflake 算法搭建发号器"></a><a href="#%E5%9F%BA%E4%BA%8E-snowflake-%E7%AE%97%E6%B3%95%E6%90%AD%E5%BB%BA%E5%8F%91%E5%8F%B7%E5%99%A8"></a> 基于 Snowflake 算法搭建发号器</h2><p>从我历年所经历的项目中，我主要使用的是 <strong>变种的 Snowflake 算法来生成业务需要的 ID 的</strong> ，本讲的重点，也是运用它去解决 ID 全局唯一性的问题。搞懂这个算法，知道它是怎么实现的，就足够你应用它来设计一套分布式发号器了，不过你可能会说了：「那你提全局唯一性，怎么不提 UUID 呢？」</p>
<p>没错，UUID（Universally Unique Identifier，通用唯一标识码）不依赖于任何第三方系统，所以在性能和可用性上都比较好，<strong>我一般会使用它生成 Request ID 来标记单次请求</strong>， 但是如果用它来作为数据库主键，它会存在以下几点问题。</p>
<p>首先，生成的 ID 做好具有单调递增性，也就是有序的，而 UUID 不具备这个特点。为什么 ID 要是有序的呢？ <strong>因为在系统设计时，ID 有可能成为排序的字段。</strong> 我给你举个例子。</p>
<p>比如，你要实现一套评论的系统时，你一般会设计两个表，一张评论表，存储评论的详细信息，其中有 ID 字段，有评论的内容，还有评论人 ID，被评论内容的 ID 等等，以 ID 字段作为分区键；另一个是评论列表，存储着内容 ID 和评论 ID 的对应关系，以内容 ID 为分区键。</p>
<p>我们在获取内容的评论列表时，需要按照时间序倒序排列，因为 ID 是时间上有序的，所以我们就可以按照评论 ID 的倒序排列。而如果评论 ID 不是在时间上有序的话，我们就需要在评论列表中再存储一个多余的创建时间的列用作排序，假设内容 ID、评论 ID 和时间都是使用 8 字节存储，我们就要多出 50% 的存储空间存储时间字段，造成了存储空间上的浪费。</p>
<p><strong>另一个原因在于 ID 有序也会提升数据的写入性能。</strong></p>
<p>我们知道 MySQL InnoDB 存储引擎使用 <code>B+ 树</code> 存储索引数据，而主键也是一种索引。索引数据在 <code>B+ 树</code> 中是有序排列的，就像下面这张图一样，图中 2，10，26 都是记录的 ID，也是索引数据。</p>
 <img src="/blog/images/pasted-51.png" width = "100%" height = "100%" align=center />

<p>这时，当插入的下一条记录的 ID 是递增的时候，比如插入 30 时，数据库只需要把它追加到后面就好了。但是如果插入的数据是无序的，比如 ID 是 13，那么数据库就要查找 13 应该插入的位置，再挪动 13 后面的数据，这就造成了多余的数据移动的开销。</p>
 <img src="/blog/images/pasted-52.png" width = "100%" height = "100%" align=center />

<p>我们知道机械磁盘在完成随机的写时，需要先做 「寻道」找到要写入的位置，也就是让磁头找到对应的磁道，这个过程是非常耗时的。<strong>而顺序写就不需要寻道，会大大提升索引的写入性能</strong> 。</p>
<p><strong>UUID 不能作为 ID 的另一个原因是它不具备业务含义，</strong> 其实现实世界中使用的 ID 中都包含有一些有意义的数据，这些数据会出现在 ID 的固定的位置上。比如说我们使用的身份证的前六位是地区编号；7～14 位是身份证持有人的生日；不同城市电话号码的区号是不同的；你从手机号码的的前三位就可以看出这个手机号隶属于哪一个运营商。而如果生成的 ID 可以被反解，那么从反解出来的信息中我们可以对 ID 来做验证，我们可以从中知道这个 ID 的生成时间，从哪个机房的发号器中生成的，为哪个业务服务的，对于问题的排查有一定的帮助。</p>
<p>最后，UUID 是由 32 个 16 进制数字组成的字符串，如果作为数据库主键使用比较耗费空间。</p>
<p>你能看到，UUID 方案有很大的局限性，也是我不建议你用它的原因，而 twitter 提出的 Snowflake 算法完全可以弥补 UUID 存在的不足，因为它不仅算法简单易实现，也满足 ID 所需要的全局唯一性，单调递增性，还包含一定的业务上的意义。</p>
<p>Snowflake 的核心思想是将 64bit 的二进制数字分成若干部分，每一部分都存储有特定含义的数据，比如说时间戳、机器 ID、序列号等等，最终生成全局唯一的有序 ID。它的标准算法是这样的：</p>
 <img src="/blog/images/pasted-53.png" width = "100%" height = "100%" align=center />

<p>从上面这张图中我们可以看到，41 位的时间戳大概可以支撑 <code>pow(2,41)/1000/60/60/24/365</code> 年，约等于 69 年，对于一个系统是足够了。</p>
<p>如果你的系统部署在多个机房，那么 10 位的机器 ID 可以继续划分为 2～3 位的 IDC 标示（可以支撑 4 个或者 8 个 IDC 机房）和 7～8 位的机器 ID（支持 128-256 台机器）；12 位的序列号代表着每个节点每毫秒最多可以生成 4096 的 ID。</p>
<p>不同公司也会依据自身业务的特点对 Snowflake 算法做一些改造，比如说减少序列号的位数增加机器 ID 的位数以支持单 IDC 更多的机器，也可以在其中加入业务 ID 字段来区分不同的业务。 <strong>比方说我现在使用的发号器的组成规则就是：</strong> 1 位兼容位恒为 0 + 41 位时间信息 + 6 位 IDC 信息（支持 64 个 IDC）+ 6 位业务信息（支持 64 个业务）+ 10 位自增信息（每毫秒支持 1024 个号）</p>
<p>我选择这个组成规则，主要是因为我在单机房只部署一个发号器的节点，并且使用 KeepAlive 保证可用性。业务信息指的是项目中哪个业务模块使用，比如用户模块生成的 ID，内容模块生成的 ID，把它加入进来，一是希望不同业务发出来的 ID 可以不同，二是因为在出现问题时可以反解 ID，知道是哪一个业务发出来的 ID。</p>
<p>那么了解了 Snowflake 算法的原理之后，我们如何把它工程化，来为业务生成全局唯一的 ID 呢？ <strong>一般来说我们会有两种算法的实现方式：</strong></p>
<p><strong>一种是嵌入到业务代码里，也就是分布在业务服务器中。</strong> 这种方案的好处是业务代码在使用的时候不需要跨网络调用，性能上会好一些，但是就需要更多的机器 ID 位数来支持更多的业务服务器。另外，由于业务服务器的数量很多，我们很难保证机器 ID 的唯一性，所以就需要引入 ZooKeeper 等分布式一致性组件来保证每次机器重启时都能获得唯一的机器 ID。</p>
<p><strong>另外一个部署方式是作为独立的服务部署，这也就是我们常说的发号器服务。</strong> 业务在使用发号器的时候就需要多一次的网络调用，但是内网的调用对于性能的损耗有限，却可以减少机器 ID 的位数，如果发号器以主备方式部署，同时运行的只有一个发号器，那么机器 ID 可以省略，这样可以留更多的位数给最后的自增信息位。即使需要机器 ID，因为发号器部署实例数有限，那么就可以把机器 ID 写在发号器的配置文件里，这样即可以保证机器 ID 唯一性，也无需引入第三方组件了。 <strong>微博和美图都是使用独立服务的方式来部署发号器的，性能上单实例单 CPU 可以达到两万每秒。</strong></p>
<p>Snowflake 算法设计的非常简单且巧妙，性能上也足够高效，同时也能够生成 <strong>具有全局唯一性、单调递增性和有业务含义的 ID</strong> ，但是它也有一些缺点，<strong>其中最大的缺点就是它依赖于系统的时间戳</strong>，一旦系统时间不准，就有可能生成重复的 ID。所以如果我们发现系统时钟不准，就可以让发号器暂时拒绝发号，直到时钟准确为止。</p>
<p>另外，如果请求发号器的 QPS 不高，比如说发号器每毫秒只发一个 ID，就会造成生成 ID 的末位永远是 1，那么在分库分表时如果使用 ID 作为分区键就会造成库表分配的不均匀。 <strong>这一点，也是我在实际项目中踩过的坑，而解决办法主要有两个：</strong></p>
<ol>
<li><p>时间戳不记录毫秒而是记录秒，这样在一个时间区间里可以多发出几个号，避免出现分库分表时数据分配不均。</p>
</li>
<li><p>生成的序列号的起始号可以做一下随机，这一秒是 21，下一秒是 30，这样就会尽量的均衡了。</p>
</li>
</ol>
<p>我在开头提到，自己的实际项目中采用的是变种的 Snowflake 算法，也就是说对 Snowflake 算法进行了一定的改造，从上面的内容中你可以看出，这些改造：</p>
<ul>
<li>一是要让算法中的 ID 生成规则符合自己业务的特点；</li>
<li>二是为了解决诸如时间回拨等问题。</li>
</ul>
<p>其实，大厂除了采取 Snowflake 算法之外，还会选用一些其他的方案，比如滴滴和美团都有提出基于数据库生成 ID 的方案。这些方法根植于公司的业务，同样能解决分布式环境下 ID 全局唯一性的问题。对你而言，可以多角度了解不同的方法，这样能够寻找到更适合自己业务目前场景的解决方案，不过我想说的是， <strong>方案不在多，而在精，方案没有最好，只有最适合，真正弄懂方法背后的原理，并将它落地，才是你最佳的选择。</strong></p>
<h2 id="小结"><a href="#小结" class="headerlink" title="# 小结"></a><a href="#%E8%AF%BE%E7%A8%8B%E5%B0%8F%E7%BB%93">#</a> 小结</h2><p>这次，我结合自己的项目经历带你了解了如何使用 Snowflake 算法解决分库分表后数据库 ID 的全局唯一的问题，在这个问题中，又延伸性地带你了解了生成的 ID 需要满足单调递增性，以及要具有一定业务含义的特性。当然，我们重点的内容是讲解如何将 Snowflake 算法落地，以及在落地过程中遇到了哪些坑，带你去解决它。</p>
<p>Snowflake 的算法并不复杂，你在使用的时候可以不考虑独立部署的问题，先想清楚按照自身的业务场景，需要如何设计 Snowflake 算法中的每一部分占的二进制位数。比如你的业务会部署几个 IDC，应用服务器要部署多少台机器，每秒钟发号个数的要求是多少等等，然后在业务代码中实现一个简单的版本先使用，等到应用服务器数量达到一定规模，再考虑独立部署的问题就可以了。这样可以避免多维护一套发号器服务，减少了运维上的复杂度。</p>
<h2 id="拓展学习"><a href="#拓展学习" class="headerlink" title=" 拓展学习"></a><a href="#%E6%8B%93%E5%B1%95%E9%98%85%E8%AF%BB"></a> 拓展学习</h2><ul>
<li><p>其他的发号器</p>
<p><a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s?__biz=MzI4NDMyNTU2Mw==&mid=2247483679&idx=1&sn=584dbd80aa08fa1188627ad725680928&mpshare=1&scene=1&srcid=1208L9z4yXKLW60rPph2ZmMn#rd">微信发号器 seqsvr (opens new window)</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/baidu/uid-generator/blob/master/README.zh_cn.md">百度 UidGenerator (opens new window)</a></p>
</li>
<li><p>同一时间位，同一机器，在生成序列号时，是要上锁的吧？</p>
<p>是的 不过像 redis 那样单线程处理就好了</p>
</li>
<li><p>snowflake 不能保证单调递增吧？首先，服务器的时钟可能有快有慢；其次，同一时刻，机器号大的机器生成的 ID 总是大于机器号小的机器，但他的请求可能是先到达了数据库。</p>
<p>首先，服务器的时钟一般是对时的，其次，如果是单独部署的发号器，没有机器 ID 是可以保证单调递增的</p>
</li>
<li><p>关于 41 位的时间戳的可支撑时间问题, 如果时间戳是从 0 开始计算则约可以支持 69 年, 但如果以当前时间开始算, 则可用的只有不到 20 年了( <code>69-(2019-1970)</code> )</p>
<p>实现时可以不以 1970 年为基准时间</p>
</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://irisalex.github.io/blog/2023/12/01/%E6%95%B0%E6%8D%AE%E7%9A%84%E5%94%AF%E4%B8%80%E6%80%A7/" data-id="cm879ko94000f3zjj5l6k9m50" data-title="数据的唯一性" class="article-share-link">Share</a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-数据优化方案2" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/blog/2023/12/01/%E6%95%B0%E6%8D%AE%E4%BC%98%E5%8C%96%E6%96%B9%E6%A1%882/" class="article-date">
  <time class="dt-published" datetime="2023-12-01T02:56:57.000Z" itemprop="datePublished">2023-12-01</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/blog/2023/12/01/%E6%95%B0%E6%8D%AE%E4%BC%98%E5%8C%96%E6%96%B9%E6%A1%882/">数据优化方案2</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="数据库优化方案-2：写入数据量增加时，如何实现分库分表？"><a href="#数据库优化方案-2：写入数据量增加时，如何实现分库分表？" class="headerlink" title="数据库优化方案 2：写入数据量增加时，如何实现分库分表？"></a><a href="#_%E4%B8%A8%E6%95%B0%E6%8D%AE%E5%BA%93%E4%BC%98%E5%8C%96%E6%96%B9%E6%A1%88-2-%E5%86%99%E5%85%A5%E6%95%B0%E6%8D%AE%E9%87%8F%E5%A2%9E%E5%8A%A0%E6%97%B6-%E5%A6%82%E4%BD%95%E5%AE%9E%E7%8E%B0%E5%88%86%E5%BA%93%E5%88%86%E8%A1%A8"></a>数据库优化方案 2：写入数据量增加时，如何实现分库分表？</h1><p>上次我们学习了在高并发下数据库的一种优化方案：读写分离，它就是依靠主从复制的技术使得数据库实现了数据复制为多份，增强了抵抗 <strong>大量并发读请求的能力</strong>，提升了数据库的查询性能的同时，也提升了数据的安全性，当某一个数据库节点，无论是主库还是从库发生故障时，我们还有其他的节点中存储着全量的数据，保证数据不会丢失。此时，你的电商系统的架构图变成了下面这样：</p>
 <img src="/blog/images/pasted-46.png" width = "100%" height = "100%" align=center />

<p>这时，公司 CEO 突然传来一个好消息，运营推广持续带来了流量，你所设计的电商系统的订单量突破了五千万，订单数据都是单表存储的，你的压力倍增，因为无论是数据库的查询还是写入性能都在下降，数据库的磁盘空间也在报警。所以，你主动分析现阶段自己需要考虑的问题，并寻求高效的解决方式，以便系统能正常运转下去。你考虑的问题主要有以下几点：</p>
<ol>
<li><p>系统正在持续不断地的发展</p>
<p>注册的用户越来越多，产生的订单越来越多，数据库中存储的数据也越来越多，单个表的数据量超过了千万甚至到了亿级别。这时即使你使用了索引，索引占用的空间也随着数据量的增长而增大，数据库就无法缓存全量的索引信息，那么就需要从磁盘上读取索引数据，就会影响到查询的性能了。 <strong>那么这时你要如何提升查询性能呢？</strong></p>
</li>
<li><p>数据量的增加也占据了磁盘的空间，数据库在备份和恢复的时间变长， <strong>你如何让数据库系统支持如此大的数据量呢？</strong></p>
</li>
<li><p>不同模块的数据，比如用户数据和用户关系数据，全都存储在一个主库中，一旦主库发生故障，所有的模块儿都会受到影响， <strong>那么如何做到不同模块的故障隔离呢？</strong></p>
</li>
<li><p>你已经知道了，在 4 核 8G 的云服务器上对 MySQL5.7 做 Benchmark，大概可以支撑 500TPS 和 10000QPS，你可以看到数据库对于写入性能要弱于数据查询的能力，那么随着系统写入请求量的增长， <strong>数据库系统如何来处理更高的并发写入请求呢？</strong></p>
</li>
</ol>
<p>这些问题你可以归纳成，<strong>数据库的写入请求量大造成的性能和可用性方面的问题</strong> ，要解决这些问题，你所采取的措施就是 <strong>对数据进行分片</strong> ，对数据进行分片，可以很好地分摊数据库的读写压力，也可以突破单机的存储瓶颈，而常见的一种方式是对数据库做 <strong>分库分表</strong> 。</p>
<p>分库分表是一个很常见的技术方案，你应该有所了解。那你会说了：「既然这个技术很普遍，而我又有所了解，那你为什么还要提及这个话题呢？」因为以我过往的经验来看，不少人会在分库分表这里踩坑，主要体现在：</p>
<ol>
<li><p>对如何使用正确的分库分表方式一知半解，没有明白使用场景和方法。比如，一些同学会在查询时不使用分区键；</p>
</li>
<li><p>分库分表引入了一些问题后，没有找到合适的解决方案。比如，会在查询时使用大量连表查询等等。</p>
</li>
</ol>
<p>本节课，我就带你解决这两个问题，从常人容易踩坑的地方，跳出来。</p>
<h2 id="如何对数据库做垂直拆分"><a href="#如何对数据库做垂直拆分" class="headerlink" title=" 如何对数据库做垂直拆分"></a><a href="#%E5%A6%82%E4%BD%95%E5%AF%B9%E6%95%B0%E6%8D%AE%E5%BA%93%E5%81%9A%E5%9E%82%E7%9B%B4%E6%8B%86%E5%88%86"></a> 如何对数据库做垂直拆分</h2><p>分库分表是一种常见的将数据分片的方式，它的基本思想是依照某一种策略将数据尽量平均的分配到多个数据库节点或者多个表中。</p>
<p>不同于主从复制时数据是全量地被拷贝到多个节点，分库分表后，每个节点只保存部分的数据，这样可以有效地减少单个数据库节点和单个数据表中存储的数据量，<strong>在解决了数据存储瓶颈的同时也能有效的提升数据查询的性能</strong> 。同时，因为数据被分配到多个数据库节点上，那么数据的写入请求也从请求单一主库变成了请求多个数据分片节点，在一定程度上也会提升并发写入的性能。</p>
<p>比如，我之前做过一个直播项目，在这个项目中，需要存储用户在直播间中发的消息以及直播间中的系统消息，你知道这些消息量极大，有些比较火的直播间有上万条留言是很常见的事儿，日积月累下来就积攒了几亿的数据，查询的性能和存储空间都扛不住了。没办法，就只能加班加点重构，启动多个数据库来分摊写入压力和容量的压力，也需要将原来单库的数据迁移到新启动的数据库节点上，好在最后成功完成分库分表和数据迁移校验工作，不过也着实花费了不少的时间和精力。</p>
<p>数据库分库分表的方式有两种：一种是垂直拆分，另一种是水平拆分。这两种方式，在我看来，掌握拆分方式是关键，理解拆分原理是内核。所以你在学习时，最好可以结合自身业务来思考。</p>
<p>垂直拆分，顾名思义就是对数据库竖着拆分，也就是将数据库的 <strong>表拆分到多个不同的数据库中</strong>。</p>
<p>垂直拆分的原则一般是按照业务类型来拆分，核心思想是专库专用，将业务耦合度比较高的表拆分到单独的库中。举个形象的例子就是在整理衣服的时候，将羽绒服、毛衣、T 恤分别放在不同的格子里。这样可以解决我在开篇提到的第三个问题：把不同的业务的数据分拆到不同的数据库节点上，这样一旦数据库发生故障时只会影响到某一个模块的功能，不会影响到整体功能，从而实现了数据层面的故障隔离。</p>
<p><strong>我还是以微博系统为例来给你说明一下。</strong></p>
<p>在微博系统中有和用户相关的表，有和内容相关的表，有和关系相关的表，这些表都存储在主库中。在拆分后，我们期望用户相关的表分拆到用户库中，内容相关的表分拆到内容库中，关系相关的表分拆到关系库中。</p>
 <img src="/blog/images/pasted-47.png" width = "100%" height = "100%" align=center />

<p>对数据库进行垂直拆分是一种偏常规的方式，这种方式其实你会比较常用，不过拆分之后，虽然可以暂时缓解存储容量的瓶颈，但并不是万事大吉，因为数据库垂直拆分后依然不能解决某一个业务模块的数据大量膨胀的问题，一旦你的系统遭遇某一个业务库的数据量暴增，在这个情况下，你还需要继续寻找可以弥补的方式。</p>
<p>比如微博关系量早已经过了千亿，单一的数据库或者数据表已经远远不能满足存储和查询的需求了， **这个时候，你需要将数据拆分到多个数据库和数据表中，也就是对数据库和数据表做水平拆分了 ** 。</p>
<h2 id="如何对数据库做水平拆分"><a href="#如何对数据库做水平拆分" class="headerlink" title=" 如何对数据库做水平拆分"></a><a href="#%E5%A6%82%E4%BD%95%E5%AF%B9%E6%95%B0%E6%8D%AE%E5%BA%93%E5%81%9A%E6%B0%B4%E5%B9%B3%E6%8B%86%E5%88%86"></a> 如何对数据库做水平拆分</h2><p>和垂直拆分的关注点不同：</p>
<ul>
<li>垂直拆分的关注点在于 <strong>业务相关性</strong>，</li>
<li>水平拆分指的是将单一数据表按照某一种规则拆分到多个数据库和多个数据表中，关注点在 <strong>数据的特点</strong>。</li>
</ul>
<p><strong>拆分的规则有下面这两种：</strong></p>
<ol>
<li><p>按照某一个字段的 <strong>哈希值</strong> 做拆分</p>
<p>这种拆分规则比较适用于实体表，比如说用户表，内容表，我们一般按照这些实体表的 ID 字段来拆分。比如说我们想把用户表拆分成 16 个库，64 张表，那么可以先对用户 ID 做哈希，哈希的目的是将 ID 尽量打散，然后再对 16 取余，这样就得到了分库后的索引值；对 64 取余，就得到了分表后的索引值。</p>
</li>
</ol>
 <img src="/blog/images/pasted-48.png" width = "100%" height = "100%" align=center />
    
<ol start="2">
<li><p>另一种比较常用的是按照某一个字段的 <strong>区间</strong> 来拆分，比较常用的是时间字段。</p>
<p>你知道在内容表里面有「创建时间」的字段，而我们也是按照时间来查看一个人发布的内容。我们可能会要看昨天的内容，也可能会看一个月前发布的内容，这时就可以按照创建时间的区间来分库分表，比如说可以把一个月的数据放入一张表中，这样在查询时就可以根据创建时间先定位数据存储在哪个表里面，再按照查询条件来查询。</p>
<p>一般来说，列表数据可以使用这种拆分方式，比如一个人一段时间的订单，一段时间发布的内容。但是这种方式可能会存在明显的热点，这很好理解嘛，你当然会更关注最近我买了什么，发了什么，所以查询的 QPS 也会更多一些，对性能有一定的影响。另外，使用这种拆分规则后，数据表要提前建立好，否则如果时间到了 2020 年元旦，DBA（Database Administrator，数据库管理员）却忘记了建表，那么 2020 年的数据就没有库表可写了，就会发生故障了。</p>
</li>
</ol>
 <img src="/blog/images/pasted-49.png" width = "100%" height = "100%" align=center />
    

<p>数据库在分库分表之后，数据的访问方式也有了极大的改变，<strong>原先只需要根据查询条件到从库中查询数据即可，现在则需要先确认数据在哪一个库表中，再到那个库表中查询数据</strong> 。这种复杂度也可以通过数据库中间件来解决，我们在上一节中已经有所讲解，这里就不再赘述了，不过，我想再次强调的是你需要对所使用数据库中间件的原理有足够的了解和足够强的运维上的把控能力。</p>
<p>不过，你要知道的是，分库分表虽然能够解决数据库扩展性的问题，但是它也给我们的使用带来了一些问题。</p>
<h2 id="解决分库分表引入的问题"><a href="#解决分库分表引入的问题" class="headerlink" title="# 解决分库分表引入的问题"></a><a href="#%E8%A7%A3%E5%86%B3%E5%88%86%E5%BA%93%E5%88%86%E8%A1%A8%E5%BC%95%E5%85%A5%E7%9A%84%E9%97%AE%E9%A2%98">#</a> 解决分库分表引入的问题</h2><p>分库分表引入的一个最大的问题就是 <strong>引入了分库分表键，也叫做分区键，</strong> 也就是我们对数据库做分库分表所依据的字段。</p>
<p>从分库分表规则中你可以看到，无论是哈希拆分还是区间段的拆分，我们首先都需要选取一个数据库字段，这带来一个问题是：我们之后所有的查询都需要带上这个字段，才能找到数据所在的库和表，否则就只能向所有的数据库和数据表发送查询命令。如果像上面说的要拆分成 16 个库和 64 张表，那么一次数据的查询会变成 <code>16*64=1024</code> 次查询，查询的性能肯定是极差的。</p>
<p><strong>当然，方法总比问题多，</strong> 针对这个问题，我们也会有一些相应的解决思路。比如，在用户库中我们使用 ID 作为分区键，这时如果需要按照昵称来查询用户时，你可以按照昵称作为分区键再做一次拆分，但是这样会极大的增加存储成本，如果以后我们还需要按照注册时间来查询时要怎么办呢，再做一次拆分吗？</p>
<p><strong>所以最合适的思路是</strong> 你要建立一个昵称和 ID 的映射表，在查询的时候要先通过昵称查询到 ID，再通过 ID 查询完整的数据，这个表也可以是分库分表的，也需要占用一定的存储空间，但是因为表中只有两个字段，所以相比重新做一次拆分还是会节省不少的空间的。</p>
<p><strong>分库分表引入的另外一个问题是一些数据库的特性在实现时可能变得很困难。</strong> 比如说多表的 join 在单库时是可以通过一个 SQL 语句完成的，但是拆分到多个数据库之后就无法跨库执行 SQL 了，不过好在我们对于 join 的需求不高，即使有也一般是把两个表的数据取出后在业务代码里面做筛选，复杂是有一些，不过是可以实现的。再比如说在未分库分表之前查询数据总数时只需要在 SQL 中执行 <code>count()</code> 即可，现在数据被分散到多个库表中，我们可能要考虑其他的方案，比方说将计数的数据单独存储在一张表中或者记录在 Redis 里面。</p>
<p>当然，虽然分库分表会对我们使用数据库带来一些不便，但是相比它所带来的扩展性和性能方面的提升，我们还是需要做的，因为，<strong>经历过分库分表后的系统，才能够突破单机的容量和请求量的瓶颈</strong> ，就比如说，我在开篇提到的我们的电商系统，它正是经历了分库分表，才会解决订单表数据量过大带来的性能衰减和容量瓶颈。</p>
<h2 id="小结"><a href="#小结" class="headerlink" title=" 小结"></a><a href="#%E8%AF%BE%E7%A8%8B%E5%B0%8F%E7%BB%93"></a> 小结</h2><p>总的来说，在面对数据库容量瓶颈和写并发量大的问题时，你可以采用垂直拆分和水平拆分来解决，不过你要注意，这两种方式虽然能够解决问题，但是也会引入诸如查询数据必须带上分区键，列表总数需要单独冗余存储等问题。</p>
<p>而且，你需要了解的是在实现分库分表过程中，数据从单库单表迁移多库多表是一件即繁杂又容易出错的事情，而且如果我们初期没有规划得当，后面要继续增加数据库数或者表数时，我们还要经历这个迁移的过程。所以，从我的经验出发，对于分库分表的原则主要有以下几点：</p>
<ol>
<li><p>如果在性能上没有瓶颈点那么就尽量不做分库分表；</p>
</li>
<li><p>如果要做，就尽量一次到位，比如说 16 库 64 表就基本能够满足为了几年内你的业务的需求。</p>
</li>
<li><p>很多的 NoSQL 数据库，例如 Hbase，MongoDB 都提供 auto sharding 的特性，如果你的团队内部对于这些组件比较熟悉，有较强的运维能力，那么也可以考虑使用这些 NoSQL 数据库替代传统的关系型数据库。</p>
</li>
</ol>
<p>其实，在我看来，有很多人并没有真正从根本上搞懂为什么要拆分，拆分后会带来哪些问题，只是一味地学习大厂现有的拆分方法，从而导致问题频出。 <strong>所以，你在使用一个方案解决一个问题的时候一定要弄清楚原理，搞清楚这个方案会带来什么问题，要如何来解决，要知其然也知其所以然，这样才能在解决问题的同时避免踩坑。</strong></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://irisalex.github.io/blog/2023/12/01/%E6%95%B0%E6%8D%AE%E4%BC%98%E5%8C%96%E6%96%B9%E6%A1%882/" data-id="cm879ko93000d3zjjawnohuss" data-title="数据优化方案2" class="article-share-link">Share</a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-数据优化方案" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/blog/2023/12/01/%E6%95%B0%E6%8D%AE%E4%BC%98%E5%8C%96%E6%96%B9%E6%A1%88/" class="article-date">
  <time class="dt-published" datetime="2023-12-01T02:49:21.000Z" itemprop="datePublished">2023-12-01</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/blog/2023/12/01/%E6%95%B0%E6%8D%AE%E4%BC%98%E5%8C%96%E6%96%B9%E6%A1%88/">数据优化方案1</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="数据库优化方案-1：查询请求增加时，如何做主从分离？"><a href="#数据库优化方案-1：查询请求增加时，如何做主从分离？" class="headerlink" title=" 数据库优化方案 1：查询请求增加时，如何做主从分离？"></a><a href="%E6%95%B0%E6%8D%AE%E5%BA%93%E4%BC%98%E5%8C%96%E6%96%B9%E6%A1%88-1-%E6%9F%A5%E8%AF%A2%E8%AF%B7%E6%B1%82%E5%A2%9E%E5%8A%A0%E6%97%B6-%E5%A6%82%E4%BD%95%E5%81%9A%E4%B8%BB%E4%BB%8E%E5%88%86%E7%A6%BB"></a> 数据库优化方案 1：查询请求增加时，如何做主从分离？</h1><p>我们用池化技术解决了数据库连接复用的问题，这时，你的垂直电商系统虽然整体架构上没有变化，但是和数据库交互的过程有了变化，在你的 Web 工程和数据库之间增加了数据库连接池，减少了频繁创建连接的成本，从上节课的测试来看性能上可以提升 80%。现在的架构图如下所示：</p>
 <img src="/blog/images/pasted-42.png" width = "100%" height = "100%" align=center />

<p>此时，你的数据库还是单机部署，依据一些云厂商的 Benchmark 的结果，<strong>在 4 核 8G 的机器上运 MySQL 5.7 时，大概可以支撑 500 的 TPS 和 10000 的 QPS</strong>。这时，运营负责人说正在准备双十一活动，并且公司层面会继续投入资金在全渠道进行推广，这无疑会引发查询量骤然增加的问题。那么今天，我们就一起来看看当查询请求增加时，应该如何做主从分离来解决问题。</p>
<h2 id="主从读写分离"><a href="#主从读写分离" class="headerlink" title=" 主从读写分离"></a><a href="#%E4%B8%BB%E4%BB%8E%E8%AF%BB%E5%86%99%E5%88%86%E7%A6%BB"></a> 主从读写分离</h2><p>其实，大部分系统的访问模型是 <strong>读多写少</strong>，读写请求量的差距可能达到几个数量级。</p>
<p>这很好理解，刷朋友圈的请求量肯定比发朋友圈的量大，淘宝上一个商品的浏览量也肯定远大于它的下单量。因此，我们优先考虑数据库如何抗住更高的查询请求，那么首先你需要把读写流量区分开，因为这样才方便针对读流量做单独的扩展，这就是我们所说的主从读写分离。</p>
<p>它其实是个流量分离的问题，就好比道路交通管制一样，一个四车道的大马路划出三个车道给领导外宾通过，另外一个车道给我们使用，优先保证领导先行，就是这个道理。</p>
<p>这个方法本身是一种常规的做法，即使在一个大的项目中，它也是一个应对数据库突发读流量的有效方法。</p>
<p>我目前的项目中就曾出现过前端流量突增导致从库负载过高的问题，DBA 兄弟会优先做一个从库扩容上去，这样对数据库的读流量就会落入到多个从库上，从库的负载就降了下来，然后研发同学再考虑使用什么样的方案将流量挡在数据库层之上。</p>
<h2 id="主从读写的两个技术关键点"><a href="#主从读写的两个技术关键点" class="headerlink" title=" 主从读写的两个技术关键点"></a><a href="#%E4%B8%BB%E4%BB%8E%E8%AF%BB%E5%86%99%E7%9A%84%E4%B8%A4%E4%B8%AA%E6%8A%80%E6%9C%AF%E5%85%B3%E9%94%AE%E7%82%B9"></a> 主从读写的两个技术关键点</h2><p>一般来说在主从读写分离机制中，我们将一个数据库的数据拷贝为一份或者多份，并且写入到其它的数据库服务器中，原始的数据库我们称为 <strong>主库</strong>，主要负责数据的写入，拷贝的目标数据库称为 <strong>从库</strong>，主要负责支持数据查询。可以看到，主从读写分离有两个技术上的关键点：</p>
<ol>
<li>一个是数据的拷贝，我们称为主从复制；</li>
<li>在主从分离的情况下，我们 <strong>如何屏蔽主从分离带来的访问数据库方式的变化</strong>，让开发同学像是在使用单一数据库一样。</li>
</ol>
<p>接下来，我们分别来看一看。</p>
<h3 id="1-主从复制"><a href="#1-主从复制" class="headerlink" title=" 1. 主从复制"></a><a href="#_1-%E4%B8%BB%E4%BB%8E%E5%A4%8D%E5%88%B6"></a> 1. 主从复制</h3><p>我先以 MySQL 为例介绍一下主从复制。</p>
<p>MySQL 的主从复制是依赖于 binlog 的，也就是记录 MySQL 上的所有变化并以二进制形式保存在磁盘上二进制日志文件。**主从复制就是将 binlog 中的数据从主库传输到从库上 ** ，一般这个过程是异步的，即主库上的操作不会等待 binlog 同步的完成。</p>
<p>**主从复制的过程是这样的： **</p>
<ol>
<li>首先从库在连接到主节点时会创建一个 IO 线程，用以请求主库更新的 binlog，并且把接收到的 binlog 信息写入一个叫做 relay log 的日志文件中</li>
<li>而主库也会创建一个 log dump 线程来发送 binlog 给从库；</li>
<li>同时，从库还会创建一个 SQL 线程读取 relay log 中的内容，并且在从库中做回放，最终实现主从的一致性。这是一种比较常见的主从复制方式。</li>
</ol>
<p>在这个方案中，使用独立的 log dump 线程是一种异步的方式，可以避免对主库的主体更新流程产生影响，而从库在接收到信息后并不是写入从库的存储中，是写入一个 relay log，是避免写入从库实际存储会比较耗时，最终造成从库和主库延迟变长。</p>
 <img src="/blog/images/pasted-43.png" width = "100%" height = "100%" align=center />

<p>你会发现，基于性能的考虑，主库的写入流程并没有等待主从同步完成就会返回结果，那么在极端的情况下，比如说主库上 binlog 还没有来得及刷新到磁盘上就出现了磁盘损坏或者机器掉电，就会导致 binlog 的丢失，最终造成主从数据的不一致。 <strong>不过，这种情况出现的概率很低，对于互联网的项目来说是可以容忍的。</strong></p>
<p>做了主从复制之后，我们就可以在写入时只写主库，在读数据时只读从库，这样即使写请求会锁表或者锁记录，也不会影响到读请求的执行。同时呢，在读流量比较大的情况下，我们可以部署多个从库共同承担读流量，这就是所说的 <strong>一主多从</strong> 部署方式，在你的垂直电商项目中就可以通过这种方式来抵御较高的并发读流量。另外，从库也可以当成一个备库来使用，以避免主库故障导致数据丢失。</p>
<p><strong>那么你可能会说，是不是我无限制地增加从库的数量就可以抵抗大量的并发呢？</strong> 实际上并不是的。因为随着从库数量增加，从库连接上来的 IO 线程比较多，<strong>主库也需要创建同样多的 log dump 线程来处理复制的请求</strong>，对于主库资源消耗比较高，同时受限于主库的网络带宽，所以在实际使用中，<strong>一般一个主库最多挂 3～5 个从库</strong>。</p>
<p><strong>当然，主从复制也有一些缺陷，</strong> 除了带来了部署上的复杂度，还有就是会带来一定的主从同步的延迟，这种延迟有时候会对业务产生一定的影响，我举个例子你就明白了。</p>
<p>在发微博的过程中会有些同步的操作，像是更新数据库的操作，也有一些异步的操作，比如说将微博的信息同步给审核系统，所以我们在更新完主库之后，会将微博的 ID 写入消息队列，再由队列处理机依据 ID 在从库中获取微博信息再发送给审核系统。 <strong>此时如果主从数据库存在延迟，会导致在从库中获取不到微博信息，整个流程会出现异常。</strong></p>
 <img src="/blog/images/pasted-44.png" width = "100%" height = "100%" align=center />

<p>这个问题解决的思路有很多，<strong>核心思想就是尽量不去从库中查询信息</strong> ，纯粹以上面的例子来说，我就有三种解决方案：</p>
<ol>
<li><p><strong>第一种方案是数据的冗余。</strong></p>
<p>你可以在发送消息队列时不仅仅发送微博 ID，而是发送队列处理机需要的所有微博信息，借此避免从数据库中重新查询数据。</p>
</li>
<li><p><strong>第二种方案是使用缓存。</strong></p>
<p>我可以在同步写数据库的同时，也把微博的数据写入到 Memcached 缓存里面，这样队列处理机在获取微博信息的时候会优先查询缓存，这样也可以保证数据的一致性。</p>
</li>
<li><p><strong>最后一种方案是查询主库。</strong></p>
<p>我可以在队列处理机中不查询从库而改为查询主库。不过，这种方式使用起来要慎重，要明确查询的量级不会很大，是在主库的可承受范围之内，否则会对主库造成比较大的压力。</p>
</li>
</ol>
<p>我会优先考虑第一种方案，因为这种方式足够简单， <strong>不过可能造成单条消息比较大，从而增加了消息发送的带宽和时间</strong> 。</p>
<p>缓存的方案比较 <strong>适合新增数据的场景</strong>，在更新数据的场景下， <strong>先更新缓存可能会造成数据的不一致</strong> ，比方说两个线程同时更新数据：</p>
<ol>
<li>线程 A 把缓存中的数据更新为 1</li>
<li>此时另一个线程 B 把缓存中的数据更新为 2，然后线程 B 又更新数据库中的数据为 2，</li>
<li>此时线程 A 更新数据库中的数据为 1，这样数据库中的值（1）和缓存中的值（2）就不一致了。</li>
</ol>
<p>最后，若非万不得已的情况下，我不会使用第三种方案。原因是这种方案要提供一个查询主库的接口，在团队开发的过程中，你很难保证其他同学不会滥用这个方法，而一旦主库承担了大量的读请求导致崩溃，那么对于整体系统的影响是极大的。</p>
<p>所以对这三种方案来说，你要有所取舍，根据实际项目情况做好选择。</p>
<p><strong>另外，主从同步的延迟，是我们排查问题时很容易忽略的一个问题。</strong> 有时候我们遇到从数据库中获取不到信息的诡异问题时，会纠结于代码中是否有一些逻辑会把之前写入的内容删除，但是你又会发现，过了一段时间再去查询时又可以读到数据了，这基本上就是主从延迟在作怪。所以，一般我们会把从库落后的时间作为一个重点的数据库指标做监控和报警，正常的时间是在毫秒级别，一旦落后的时间达到了秒级别就需要告警了。</p>
<h3 id="2-如何访问数据库"><a href="#2-如何访问数据库" class="headerlink" title=" 2. 如何访问数据库"></a><a href="#_2-%E5%A6%82%E4%BD%95%E8%AE%BF%E9%97%AE%E6%95%B0%E6%8D%AE%E5%BA%93"></a> 2. 如何访问数据库</h3><p>我们已经使用主从复制的技术将数据复制到了多个节点，也实现了数据库读写的分离，这时，对于数据库的使用方式发生了变化。以前只需要使用一个数据库地址就好了，现在需要使用一个主库地址和多个从库地址，并且需要区分写入操作和查询操作，如果结合下一节课中要讲解的内容 <strong>分库分表</strong>，复杂度会提升更多。 <strong>为了降低实现的复杂度，业界涌现了很多数据库中间件来解决数据库的访问问题，这些中间件可以分为两类。</strong></p>
<ol>
<li><p>第一类以淘宝的 TDDL（ Taobao Distributed Data Layer）为代表，<strong>以代码形式内嵌运行在应用程序内部</strong>。</p>
<p>你可以把它看成是一种数据源的代理，它的配置管理着多个数据源，每个数据源对应一个数据库，可能是主库，可能是从库。当有一个数据库请求时，中间件将 SQL 语句发给某一个指定的数据源来处理，然后将处理结果返回。</p>
<p>这一类中间件的优点是简单易用，没有多余的部署成本，因为它是植入到应用程序内部，与应用程序一同运行的，所以比较适合运维能力较弱的小团队使用；缺点是缺乏多语言的支持，目前业界这一类的主流方案除了 TDDL，还有早期的网易 DDB，它们都是 Java 语言开发的，无法支持其他的语言。另外，版本升级也依赖使用方更新，比较困难。</p>
</li>
<li><p>另一类是单独部署的代理层方案</p>
<p>这一类方案代表比较多，如早期阿里巴巴开源的 Cobar，基于 Cobar 开发出来的 Mycat，360 开源的 Atlas，美团开源的基于 Atlas 开发的 DBProxy 等等。</p>
<p>这一类中间件部署在独立的服务器上，业务代码如同在使用单一数据库一样使用它，实际上它内部管理着很多的数据源，当有数据库请求时，它会对 SQL 语句做必要的改写，然后发往指定的数据源。</p>
<p>它一般使用标准的 MySQL 通信协议，所以可以很好地支持多语言。由于它是独立部署的，所以也比较方便进行维护升级，比较适合有一定运维能力的大中型团队使用。它的缺陷是所有的 SQL 语句都需要跨两次网络：从应用到代理层和从代理层到数据源，所以在性能上会有一些损耗。</p>
</li>
</ol>
 <img src="/blog/images/pasted-45.png" width = "100%" height = "100%" align=center />
    

<p>这些中间件，对你而言，可能并不陌生，但是我想让你注意到是， <strong>在使用任何中间件的时候一定要保证对于中间件有足够深入的了解，否则一旦出了问题没法快速地解决就悲剧了。</strong></p>
<p>**我之前的一个项目中， ** 一直使用自研的一个组件来实现分库分表，后来发现这套组件有一定几率会产生对数据库多余的连接，于是团队讨论后决定替换成 Sharding-JDBC。原本以为是一次简单的组件切换，结果上线后发现两个问题：</p>
<ul>
<li>一是因为使用姿势不对，会偶发地出现分库分表不生效导致扫描所有库表的情况，</li>
<li>二是偶发地出现查询延时达到秒级别。</li>
</ul>
<p>由于缺少对于 Sharding-JDBC 足够的了解，这两个问题我们都没有很快解决，后来不得已只能切回原来的组件，在找到问题之后再进行切换。</p>
<h2 id="小结"><a href="#小结" class="headerlink" title=" 小结"></a><a href="#%E8%AF%BE%E7%A8%8B%E5%B0%8F%E7%BB%93"></a> 小结</h2><p>这次带你了解了查询量增加时，我们如何通过主从分离和一主多从部署抵抗增加的数据库流量的，你除了掌握主从复制的技术之外，还需要 <strong>了解主从分离会带来什么问题以及它们的解决办法</strong> 。这里我想让你明确的要点主要有：</p>
<ol>
<li><p>主从读写分离以及部署一主多从可以解决突发的数据库读流量，是一种数据库 <strong>横向扩展</strong> 的方法；</p>
</li>
<li><p>读写分离后，<strong>主从的延迟是一个关键的监控指标</strong>，可能会造成写入数据之后立刻读的时候读取不到的情况；</p>
</li>
<li><p>业界有很多的方案可以屏蔽主从分离之后数据库访问的细节，让开发人员像是访问单一数据库一样，包括有像 TDDL、Sharding-JDBC 这样的嵌入应用内部的方案，也有像 Mycat 这样的独立部署的代理方案。</p>
</li>
</ol>
<p>其实，**我们可以把主从复制引申为存储节点之间互相复制存储数据的技术 ** ，它可以实现数据的冗余，以达到备份和提升横向扩展能力的作用。在使用主从复制这个技术点时，你一般会考虑两个问题：</p>
<ol>
<li><p>主从的一致性和写入性能的权衡</p>
<p>如果你要保证所有从节点都写入成功，那么写入性能一定会受影响；</p>
<p>如果你只写入主节点就返回成功，那么从节点就有可能出现数据同步失败的情况，从而造成主从不一致， <strong>而在互联网的项目中，我们一般会优先考虑性能而不是数据的强一致性。</strong></p>
</li>
<li><p>主从的延迟问题</p>
<p>很多诡异的读取不到数据的问题都可能会和它有关，如果你遇到这类问题不妨先看看主从延迟的数据。</p>
</li>
</ol>
<p>我们采用的很多组件都会使用到这个技术，比如，</p>
<ul>
<li>Redis 也是通过主从复制实现读写分离；</li>
<li>Elasticsearch 中存储的索引分片也可以被复制到多个节点中；</li>
<li>写入到 HDFS 中文件也会被复制到多个 DataNode 中。</li>
</ul>
<p>只是不同的组件对于复制的一致性、延迟要求不同，采用的方案也不同。 <strong>但是这种设计的思想是通用的，是你需要了解的，这样你在学习其他存储组件的时候就能够触类旁通了。</strong></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://irisalex.github.io/blog/2023/12/01/%E6%95%B0%E6%8D%AE%E4%BC%98%E5%8C%96%E6%96%B9%E6%A1%88/" data-id="cm879ko93000b3zjjh05783aj" data-title="数据优化方案1" class="article-share-link">Share</a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-池化技术" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/blog/2023/12/01/%E6%B1%A0%E5%8C%96%E6%8A%80%E6%9C%AF/" class="article-date">
  <time class="dt-published" datetime="2023-12-01T01:00:19.000Z" itemprop="datePublished">2023-12-01</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/blog/2023/12/01/%E6%B1%A0%E5%8C%96%E6%8A%80%E6%9C%AF/">池化技术</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="池化技术：如何减少频繁创建数据库连接的性能损耗？"><a href="#池化技术：如何减少频繁创建数据库连接的性能损耗？" class="headerlink" title="池化技术：如何减少频繁创建数据库连接的性能损耗？"></a>池化技术：如何减少频繁创建数据库连接的性能损耗？</h1><p>来想象这样一个场景，一天，公司 CEO 把你叫到会议室，告诉你公司看到了一个新的商业机会，希望你能带领一名兄弟，迅速研发出一套面向某个垂直领域的电商系统。</p>
<p>在人手紧张，时间不足的情况下，为了能够完成任务，你毫不犹豫地采用了 <strong>最简单的架构</strong> ：前端一台 Web 服务器运行业务代码，后端一台数据库服务器存储业务数据。</p>
 <img src="/blog/images/pasted-39.png" width = "100%" height = "100%" align=center />

<p>这个架构图是我们每个人最熟悉的，最简单的架构原型，很多系统在一开始都是长这样的，只是随着业务复杂度的提高，架构做了叠加，然后看起来就越来越复杂了。</p>
<p>再说回我们的垂直电商系统，系统一开始上线之后，虽然用户量不大，但运行平稳，你很有成就感，不过 CEO 觉得用户量太少了，所以紧急调动运营同学做了一次全网的流量推广。</p>
<p>这一推广很快带来了一大波流量， <strong>但这时，系统的访问速度开始变慢。</strong></p>
<p>分析程序的日志之后，你发现系统慢的原因 <strong>出现在和数据库的交互上</strong> 。因为你们数据库的调用方式是先获取数据库的连接，然后依靠这条连接从数据库中查询数据，最后关闭连接释放数据库资源。这种调用方式下，每次执行 SQL 都需要重新建立连接，所以你怀疑， <strong>是不是频繁地建立数据库连接耗费时间长导致了访问慢的问题</strong>。</p>
<p><strong>那么为什么频繁创建连接会造成响应时间慢呢？来看一个实际的测试。</strong></p>
<p>我用</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -i: 指定网卡</span></span><br><span class="line">tcpdump -i bond0 -nn -tttt port 4490</span><br><span class="line">  </span><br><span class="line">        Copied!</span><br><span class="line">    </span><br></pre></td></tr></table></figure>

<p>命令抓取了线上 MySQL 建立连接的网络包来做分析，从抓包结果来看，整个 MySQL 的连接过程可以分为两部分：</p>
 <img src="/blog/images/pasted-40.png" width = "100%" height = "100%" align=center /> 

<ul>
<li><p><strong>第一部分是前三个数据包</strong></p>
<p>第一个数据包是客户端向服务端发送的一个 SYN 包，</p>
<p>第二个包是服务端回给客户端的 ACK 包以及一个 SYN 包，</p>
<p>第三个包是客户端回给服务端的 ACK 包，熟悉 TCP 协议的同学可以看出这是一个 TCP 的三次握手过程。</p>
</li>
<li><p><strong>第二部分是 MySQL 服务端校验客户端密码的过程。</strong></p>
<p>其中第一个包是服务端发给客户端要求认证的报文，</p>
<p>第二和第三个包是客户端将加密后的密码发送给服务端的包，</p>
<p>最后两个包是服务端回给客户端认证 OK 的报文。</p>
</li>
</ul>
<p>从图中，你可以看到整个连接过程大概消耗了 4ms（969012-964904）。</p>
<p>那么单条 SQL 执行时间是多少呢？我们统计了一段时间的 SQL 执行时间，发现 SQL 的平均执行时间大概是 1ms，也就是说相比于 SQL 的执行，MySQL 建立连接的过程是比较耗时的。这在请求量小的时候其实影响不大，因为无论是建立连接还是执行 SQL，耗时都是毫秒级别的。可是请求量上来之后，如果按照原来的方式建立一次连接只执行一条 SQL 的话，1s 只能执行 200 次数据库的查询，而数据库建立连接的时间占了其中 4&#x2F;5。</p>
<p><strong>那这时你要怎么做呢？</strong></p>
<p>一番谷歌搜索之后，你发现解决方案也很简单，只要使用连接池将数据库连接预先建立好，这样在使用的时候就不需要频繁地创建连接了。调整之后，你发现 1s 就可以执行 1000 次的数据库查询，查询性能大大的提升了。</p>
<h2 id="用连接池预先建立数据库连接-用连接池预先建立数据库连接"><a href="#用连接池预先建立数据库连接-用连接池预先建立数据库连接" class="headerlink" title="(用连接池预先建立数据库连接) 用连接池预先建立数据库连接"></a>(用连接池预先建立数据库连接) 用连接池预先建立数据库连接</h2><p>虽然短时间解决了问题，不过你还是想彻底搞明白解决问题的核心原理，于是又开始补课。</p>
<p>其实，在开发过程中我们会用到很多的连接池，像是数据库连接池、HTTP 连接池、Redis 连接池等等。而连接池的管理是连接池设计的核心， <strong>我就以数据库连接池为例，来说明一下连接池管理的关键点。</strong></p>
<p>数据库连接池有两个最重要的配置： <strong>最小连接数和最大连接数，</strong> 它们控制着从连接池中获取连接的流程：</p>
<ul>
<li>如果当前连接数小于最小连接数，则创建新的连接处理数据库请求；</li>
<li>如果连接池中有空闲连接则复用空闲连接；</li>
<li>如果空闲池中没有连接并且当前连接数小于最大连接数，则创建新的连接处理请求；</li>
<li>如果当前连接数已经大于等于最大连接数，则按照配置中设定的时间（C3P0 的连接池配置是 checkoutTimeout）等待旧的连接可用；</li>
<li>如果等待超过了这个设定时间则向用户抛出错误。</li>
</ul>
<p>这个流程你不用死记，非常简单。你可以停下来想想如果你是连接池的设计者你会怎么设计，有哪些关键点，这个设计思路在我们以后的架构设计中经常会用到。</p>
<p>为了方便你理解性记忆这个流程，我来举个例子。</p>
<p>假设你在机场里经营着一家按摩椅的小店，店里一共摆着 10 台按摩椅（类比最大连接数），为了节省成本（按摩椅费电），你平时会保持店里开着 4 台按摩椅（最小连接数），其他 6 台都关着。</p>
<p>有顾客来的时候，如果平时保持启动的 4 台按摩椅有空着的，你直接请他去空着的那台就好了。但如果顾客来的时候，4 台按摩椅都不空着，那你就会新启动一台，直到你的 10 台按摩椅都被用完。</p>
<p>那 10 台按摩椅都被用完之后怎么办呢？你会告诉用户，稍等一会儿，我承诺你 5 分钟（等待时间）之内必定能空出来，然后第 11 位用户就开始等着。这时，会有两个结果：如果 5 分钟之内有空出来的，那顾客直接去空出来的那台按摩椅就可以了，但如果用户等了 5 分钟都没空出来，那你就得赔礼道歉，让用户去其他店再看看。</p>
<p>对于数据库连接池，根据我的经验，一般在线上我建议最小连接数控制在 10 左右，最大连接数控制在 20～30 左右即可。</p>
<p>在这里，你需要注意池子中连接的维护问题，也就是我提到的按摩椅。有的按摩椅虽然开着，但有的时候会有故障，一般情况下， <strong>按摩椅故障</strong> 的原因可能有以下几种：</p>
<ol>
<li><p>数据库的域名对应的 IP 发生了变更，池子的连接还是使用旧的 IP，当旧的 IP 下的数据库服务关闭后，再使用这个连接查询就会发生错误；</p>
</li>
<li><p>MySQL 有个参数是 <code>wait_timeout</code>，控制着当数据库连接闲置多长时间后，数据库会主动的关闭这条连接。这个机制对于数据库使用方是无感知的，所以当我们使用这个被关闭的连接时就会发生错误。</p>
</li>
</ol>
<p>那么，作为按摩椅店老板，你怎么保证你启动着的按摩椅一定是可用的呢？</p>
<ol>
<li><p>启动一个线程来定期检测连接池中的连接是否可用，比如使用连接发送 <code>select 1</code> 的命令给数据库看是否会抛出异常，如果抛出异常则将这个连接从连接池中移除，并且尝试关闭。目前 C3P0 连接池可以采用这种方式来检测连接是否可用， <strong>也是我比较推荐的方式。</strong></p>
</li>
<li><p>在获取到连接之后，先校验连接是否可用，如果可用才会执行 SQL 语句。比如 DBCP 连接池的 <code>testOnBorrow</code> 配置项，就是控制是否开启这个验证。这种方式在获取连接时会引入多余的开销， <strong>在线上系统中还是尽量不要开启，在测试服务上可以使用。</strong></p>
</li>
</ol>
<p>至此，你彻底搞清楚了连接池的工作原理。可是，当你刚想松一口气的时候，CEO 又提出了一个新的需求。你分析了一下这个需求，发现在一个非常重要的接口中，你需要访问 3 次数据库。根据经验判断，你觉得这里未来肯定会成为系统瓶颈。</p>
<p>进一步想，你觉得可以创建多个线程来并行处理与数据库之间的交互，这样速度就能快了。不过，因为有了上次数据库的教训，你想到在高并发阶段，频繁创建线程的开销也会很大，于是顺着之前的思路继续想，猜测到了线程池。</p>
<h2 id="用线程池预先创建线程-用线程池预先创建线程"><a href="#用线程池预先创建线程-用线程池预先创建线程" class="headerlink" title="(用线程池预先创建线程) 用线程池预先创建线程"></a>(用线程池预先创建线程) 用线程池预先创建线程</h2><p>果不其然，JDK 1.5 中引入的 <code>ThreadPoolExecutor</code> 就是一种线程池的实现，它有两个重要的参数：<code>coreThreadCount</code> 和 <code>maxThreadCount</code>，这两个参数控制着线程池的执行过程。它的执行原理类似上面我们说的按摩椅店的模式，我这里再给你描述下，以加深你的记忆：</p>
<ul>
<li>如果线程池中的线程数少于 coreThreadCount 时，处理新的任务时会创建新的线程；</li>
<li>如果线程数大于 coreThreadCount 则把任务丢到一个队列里面，由当前空闲的线程执行；</li>
<li>当队列中的任务堆积满了的时候，则继续创建线程，直到达到 maxThreadCount；</li>
<li>当线程数达到 maxTheadCount 时还有新的任务提交，那么我们就不得不将它们丢弃了。</li>
</ul>
 <img src="/blog/images/pasted-41.png" width = "100%" height = "100%" align=center />

<p>这个任务处理流程看似简单，实际上有很多坑，你在使用的时候一定要注意。</p>
<p><strong>首先，</strong> JDK 实现的这个线程池优 <strong>先把任务放入队列暂存起来，而不是创建更多的线程</strong> ，它比较适用于执行 CPU 密集型的任务，也就是需要执行大量 CPU 运算的任务。这是为什么呢？因为执行 CPU 密集型的任务时 CPU 比较繁忙，因此只需要创建和 CPU 核数相当的线程就好了，多了反而会造成线程上下文切换，降低任务执行效率。所以当当前线程数超过核心线程数时，线程池不会增加线程，而是放在队列里等待核心线程空闲下来。</p>
<p>但是，我们平时开发的 Web 系统通常都有大量的 IO 操作，比方说查询数据库、查询缓存等等。任务在执行 IO 操作的时候 CPU 就空闲了下来，这时如果增加执行任务的线程数而不是把任务暂存在队列中，就可以在单位时间内执行更多的任务，大大提高了任务执行的吞吐量。所以你看 Tomcat 使用的线程池就不是 JDK 原生的线程池，而是做了一些改造，当线程数超过 coreThreadCount 之后会优先创建线程，直到线程数到达 maxThreadCount，这样就比较适合于 Web 系统大量 IO 操作的场景了，你在实际运用过程中也可以参考借鉴。</p>
<p><strong>其次，线程池中使用的队列的堆积量也是我们需要监控的重要指标</strong> ，对于实时性要求比较高的任务来说，这个指标尤为关键。</p>
<p><strong>我在实际项目中就曾经遇到过任务被丢给线程池之后，长时间都没有被执行的诡异问题。</strong> 最初，我认为这是代码的 Bug 导致的，后来经过排查发现，是因为线程池的 coreThreadCount 和 maxThreadCount 设置的比较小，导致任务在线程池里面大量的堆积，在调大了这两个参数之后问题就解决了。跳出这个坑之后，我就把重要线程池的队列任务堆积量 ，作为一个重要的监控指标放到了系统监控大屏上。</p>
<p><strong>最后，</strong> 如果你使用线程池请一定记住 <strong>不要使用无界队列（即没有设置固定大小的队列）</strong> 。也许你会觉得使用了无界队列后，任务就永远不会被丢弃，只要任务对实时性要求不高，反正早晚有消费完的一天。但是，大量的任务堆积会占用大量的内存空间，一旦内存空间被占满就会频繁地触发 Full GC，造成服务不可用，我之前排查过的一次 GC 引起的宕机，起因就是系统中的一个线程池使用了无界队列。</p>
<p>理解了线程池的关键要点，你在系统里加上了这个特性，至此，系统稳定，你圆满完成了公司给你的研发任务。</p>
<p>这时，你回顾一下这两种技术，会发现它们都有一个 <strong>共同点：</strong> 它们所管理的对象，无论是连接还是线程，<strong>它们的创建过程都比较耗时，也比较消耗系统资源</strong> 。所以，我们把它们放在一个池子里统一管理起来，以达到 <strong>提升性能和资源复用的目的</strong> 。</p>
<p><strong>这是一种常见的软件设计思想，叫做池化技术，</strong> 它的核心思想是空间换时间，期望使用预先创建好的对象来减少频繁创建对象的性能开销，同时还可以对对象进行统一的管理，降低了对象的使用的成本，总之是好处多多。</p>
<p>不过，池化技术也存在一些缺陷，比方说存储池子中的对象肯定需要消耗多余的内存，如果对象没有被频繁使用，就会造成内存上的浪费。再比方说，池子中的对象需要在系统启动的时候就预先创建完成，这在一定程度上增加了系统启动时间。</p>
<p>可这些缺陷相比池化技术的优势来说就比较微不足道了，只要我们确认要使用的对象在创建时确实比较耗时或者消耗资源，并且这些对象也确实会被频繁地创建和销毁，我们就可以使用池化技术来优化。</p>
<h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>我们模拟了研发垂直电商系统最原始的场景，在遇到数据库查询性能下降的问题时，我们使用数据库连接池解决了频繁创建连接带来的性能问题，后面又使用线程池提升了并行查询数据库的性能。</p>
<p>其实，连接池和线程池你并不陌生，不过你可能对它们的原理和使用方式上还存在困惑或者误区，我在面试时，就发现有很多的同学对线程池的基本使用方式都不了解。我想再次强调的重点是：</p>
<ul>
<li>池子的最大值和最小值的设置很重要，初期可以依据经验来设置，后面还是需要根据实际运行情况做调整。</li>
<li>池子中的对象需要在使用之前预先初始化完成，这叫做 <strong>池子的预热</strong> ，比方说使用线程池时就需要预先初始化所有的核心线程。如果池子未经过预热可能会导致系统重启后产生比较多的慢请求。</li>
<li>池化技术核心是一种空间换时间优化方法的实践，所以要关注空间占用情况，避免出现空间过度使用出现内存泄露或者频繁垃圾回收等问题。</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://irisalex.github.io/blog/2023/12/01/%E6%B1%A0%E5%8C%96%E6%8A%80%E6%9C%AF/" data-id="cm879ko94000h3zjj4o6p49g7" data-title="池化技术" class="article-share-link">Share</a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-系统扩展性" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/blog/2023/11/29/%E7%B3%BB%E7%BB%9F%E6%89%A9%E5%B1%95%E6%80%A7/" class="article-date">
  <time class="dt-published" datetime="2023-11-29T11:24:54.000Z" itemprop="datePublished">2023-11-29</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/blog/2023/11/29/%E7%B3%BB%E7%BB%9F%E6%89%A9%E5%B1%95%E6%80%A7/">系统扩展性</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="系统的高扩展性"><a href="#系统的高扩展性" class="headerlink" title="系统的高扩展性"></a><a href="#%E7%B3%BB%E7%BB%9F%E7%9A%84%E9%AB%98%E6%89%A9%E5%B1%95%E6%80%A7" title="系统的高扩展性"></a>系统的高扩展性</h1><p>从架构设计上来说，高可扩展性是一个设计的指标，它表示可以通过增加机器的方式来线性提高系统的处理能力，从而承担更高的流量和并发 。</p>
<p>一般大家会有这样的一个疑问在架构设计之初，为什么不预先考虑好使用多少台机器，支持现有的并发呢？这个问题的答案 是峰值的流量不可控。</p>
<p>一般来说，基于成本考虑，在业务平稳期，我们会预留 30%～50% 的冗余以应对运营活动或者推广可能带来的峰值流量，但是当有一个突发事件发生时，流量可能瞬间提升到 2～3 倍甚至更高，我们以微博为例。</p>
<p>鹿晗和关晓彤互圈公布恋情，大家会到两个人的微博下面，或围观，或互动，微博的流量短时间内增长迅速，微博信息流也短暂出现无法刷出新的消息的情况。</p>
<p>那我们要如何应对突发的流量呢？架构的改造已经来不及了，最快的方式就是堆机器。不过我们需要保证，扩容了三倍的机器之后，相应的我们的系统也能支撑三倍的流量。有的人可能会产生疑问：这不是显而易见的吗？很简单啊。真的是这样吗？我们来看看做这件事儿难在哪儿。</p>
<h3 id="提升扩展性会很复杂"><a href="#提升扩展性会很复杂" class="headerlink" title="提升扩展性会很复杂"></a><a href="#%E6%8F%90%E5%8D%87%E6%89%A9%E5%B1%95%E6%80%A7%E4%BC%9A%E5%BE%88%E5%A4%8D%E6%9D%82" title="提升扩展性会很复杂"></a>提升扩展性会很复杂</h3><hr>
<p>在单机系统中通过增加处理核心的方式，来增加系统的并行处理能力，但这个方式并不总生效。因为当并行的任务数较多时，系统会因为争抢资源而达到性能上的拐点，系统处理能力不升反降。</p>
<p>而对于由多台机器组成的集群系统来说也是如此。集群系统中，不同的系统分层上可能存在一些 「瓶颈点」，这些瓶颈点制约着系统的横线扩展能力。这句话比较抽象，我举个例子你就明白了。</p>
<p>比方说，你系统的流量是每秒 1000 次请求，对数据库的请求量也是每秒 1000 次。如果流量增加 10 倍，虽然系统可以通过扩容正常服务，数据库却成了瓶颈。再比方说，单机网络带宽是 50Mbps，那么如果扩容到 30 台机器，前端负载均衡的带宽就超过了千兆带宽的限制，也会成为瓶颈点。那么，我们的系统中存在哪些服务会成为制约系统扩展的重要因素呢？</p>
<p>其实，无状态的服务和组件更易于扩展，而像 MySQL 这种存储服务是有状态的，就比较难以扩展。因为向存储集群中增加或者减少机器时，会涉及大量数据的迁移，而一般传统的关系型数据库都不支持。这就是为什么提升系统扩展性会很复杂的主要原因。</p>
<p>除此之外，从例子中你可以看到，我们需要站在整体架构的角度，而不仅仅是业务服务器的角度来考虑系统的扩展性 。 所以说，数据库、缓存、依赖的第三方、负载均衡、交换机带宽等等 都是系统扩展时需要考虑的因素。我们要知道系统并发到了某一个量级之后，哪一个因素会成为我们的瓶颈点，从而针对性地进行扩展。</p>
<h3 id="高可扩展性的设计思路"><a href="#高可扩展性的设计思路" class="headerlink" title="高可扩展性的设计思路"></a><a href="#%E9%AB%98%E5%8F%AF%E6%89%A9%E5%B1%95%E6%80%A7%E7%9A%84%E8%AE%BE%E8%AE%A1%E6%80%9D%E8%B7%AF" title="高可扩展性的设计思路"></a>高可扩展性的设计思路</h3><hr>
<p>拆分 是提升系统扩展性最重要的一个思路，它会把庞杂的系统拆分成独立的，有单一职责的模块。相对于大系统来说，考虑一个一个小模块的扩展性当然会简单一些。将复杂的问题简单化，这就是我们的思路。</p>
<p>但对于不同类型的模块，我们在拆分上遵循的原则是不一样的。我给你举一个简单的例子，假如你要设计一个社区，那么社区会有几个模块呢？可能有 5 个模块。</p>
<ul>
<li>用户：负责维护社区用户信息，注册，登陆等；</li>
<li>关系：用户之间关注、好友、拉黑等关系的维护；</li>
<li>内容：社区发的内容，就像朋友圈或者微博的内容；</li>
<li>评论、赞：用户可能会有的两种常规互动操作；</li>
<li>搜索：用户的搜索，内容的搜索。</li>
</ul>
<p>而部署方式遵照最简单的三层部署架构，负载均衡负责请求的分发，应用服务器负责业务逻辑的处理，数据库负责数据的存储落地。这时，所有模块的业务代码都混合在一起了，数据也都存储在一个库里。</p>
 <img src="/blog/images/pasted-33.png" width = "100%" height = "100%" align=center />

<h4 id="存储层的扩展性"><a href="#存储层的扩展性" class="headerlink" title="存储层的扩展性"></a><a href="#%E5%AD%98%E5%82%A8%E5%B1%82%E7%9A%84%E6%89%A9%E5%B1%95%E6%80%A7" title="存储层的扩展性"></a>存储层的扩展性</h4><p>无论是存储的数据量，还是并发访问量，不同的业务模块之间的量级相差很大，比如说成熟社区中，关系的数据量是远远大于用户数据量的，但是用户数据的访问量却远比关系数据要大。所以假如存储目前的瓶颈点是容量，那么我们只需要针对关系模块的数据做拆分就好了，而不需要拆分用户模块的数据。 所以存储拆分首先考虑的维度是业务维度。</p>
<p>拆分之后，这个简单的社区系统就有了用户库、内容库、评论库、点赞库和关系库。这么做还能隔离故障，某一个库「挂了」不会影响到其它的数据库。</p>
 <img src="/blog/images/pasted-34.png" width = "100%" height = "100%" align=center />

<p>按照业务拆分，在一定程度上提升了系统的扩展性，但系统运行时间长了之后，单一的业务数据库在容量和并发请求量上仍然会超过单机的限制。 这时，我们就需要针对数据库做第二次拆分。</p>
<p>这次拆分是按照数据特征做水平的拆分 ，比如说我们可以给用户库增加两个节点，然后按照某些算法将用户的数据拆分到这三个库里面，具体的算法我会在后面讲述数据库分库分表时和你细说。</p>
<p>水平拆分之后，我们就可以让数据库突破单机的限制了。但这里要注意，我们不能随意地增加节点，因为一旦增加节点就需要手动地迁移数据，成本还是很高的。所以基于长远的考虑，我们最好一次性增加足够的节点以避免频繁地扩容。</p>
<p>当数据库按照业务和数据维度拆分之后，我们 尽量不要使用事务。因为当一个事务中同时更新不同的数据库时，需要使用二阶段提交，来协调所有数据库要么全部更新成功，要么全部更新失败。这个协调的成本会随着资源的扩展不断升高，最终达到无法承受的程度。</p>
<h4 id="业务层的扩展性"><a href="#业务层的扩展性" class="headerlink" title="业务层的扩展性"></a><a href="#%E4%B8%9A%E5%8A%A1%E5%B1%82%E7%9A%84%E6%89%A9%E5%B1%95%E6%80%A7" title="业务层的扩展性"></a>业务层的扩展性</h4><p>我们一般会从三个维度考虑业务层的拆分方案，它们分别是：业务纬度 ，重要性纬度 和 请求来源纬度。</p>
<p>首先，我们需要把相同业务的服务拆分成单独的业务池，比方说上面的社区系统中，我们可以按照业务的维度拆分成用户池、内容池、关系池、评论池、点赞池和搜索池。</p>
<p>每个业务依赖独自的数据库资源，不会依赖其它业务的数据库资源。这样当某一个业务的接口成为瓶颈时，我们只需要扩展业务的池子，以及确认上下游的依赖方就可以了，这样就大大减少了扩容的复杂度。</p>
 <img src="/blog/images/pasted-35.png" width = "100%" height = "100%" align=center />

<p>除此之外，我们还可以根据业务接口的重要程度，把业务分为核心池和非核心池 （池子就是一组机器组成的集群） 。打个比方，就关系池而言，关注、取消关注接口相对重要一些，可以放在核心池里面；拉黑和取消拉黑的操作就相对不那么重要，可以放在非核心池里面。这样，我们可以优先保证核心池的性能，当整体流量上升时优先扩容核心池，降级部分非核心池的接口，从而保证整体系统的稳定性。</p>
 <img src="/blog/images/pasted-36.png" width = "100%" height = "100%" align=center />

<p>最后，你还可以根据接入客户端类型的不同做业务池的拆分。比如说，服务于客户端接口的业务可以定义为外网池，服务于小程序或者 HTML5 页面的业务可以定义为 H5 池，服务于内部其它部门的业务可以定义为内网池，等等。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://irisalex.github.io/blog/2023/11/29/%E7%B3%BB%E7%BB%9F%E6%89%A9%E5%B1%95%E6%80%A7/" data-id="cm879ko95000j3zjj0p3f7j8i" data-title="系统扩展性" class="article-share-link">Share</a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-系统高可用" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/blog/2023/11/29/%E7%B3%BB%E7%BB%9F%E9%AB%98%E5%8F%AF%E7%94%A8/" class="article-date">
  <time class="dt-published" datetime="2023-11-29T11:22:32.000Z" itemprop="datePublished">2023-11-29</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/blog/2023/11/29/%E7%B3%BB%E7%BB%9F%E9%AB%98%E5%8F%AF%E7%94%A8/">系统高可用怎么做</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="系统怎样做到高可用"><a href="#系统怎样做到高可用" class="headerlink" title="系统怎样做到高可用"></a><a href="#%E7%B3%BB%E7%BB%9F%E6%80%8E%E6%A0%B7%E5%81%9A%E5%88%B0%E9%AB%98%E5%8F%AF%E7%94%A8" title="系统怎样做到高可用"></a>系统怎样做到高可用</h1><p>高可用性（High Availability，HA）是你在系统设计时经常会听到的一个名词，它指的是系统具备较高的无故障运行的能力。</p>
<p>我们在很多开源组件的文档中看到的 HA 方案就是提升组件可用性，让系统免于宕机无法服务的方案。比如，你知道 Hadoop 1.0 中的 NameNode 是单点的，一旦发生故障则整个集群就会不可用；而在 Hadoop2 中提出的 NameNode HA 方案就是同时启动两个 NameNode，一个处于 Active 状态，另一个处于 Standby 状态，两者共享存储，一旦 Active NameNode 发生故障，则可以将 Standby NameNode 切换成 Active 状态继续提供服务，这样就增强了 Hadoop 的持续无故障运行的能力，也就是提升了它的可用性。</p>
<p>通常来讲，一个高并发大流量的系统，系统出现故障比系统性能低更损伤用户的使用体验。想象一下，一个日活用户过百万的系统，一分钟的故障可能会影响到上千的用户。而且随着系统日活的增加，一分钟的故障时间影响到的用户数也随之增加，系统对于可用性的要求也会更高。所以今天，我就带你了解一下在高并发下，我们如何来保证系统的高可用性，以便给你的系统设计提供一些思路。</p>
<h3 id="可用性的度量"><a href="#可用性的度量" class="headerlink" title="可用性的度量"></a><a href="#%E5%8F%AF%E7%94%A8%E6%80%A7%E7%9A%84%E5%BA%A6%E9%87%8F" title="可用性的度量"></a>可用性的度量</h3><hr>
<p>可用性是一个抽象的概念，你需要知道要如何来度量它，与之相关的概念是： MTBF 和 MTTR。</p>
<p><strong>MTBF（Mean Time Between Failure）</strong>是平均故障间隔的意思，代表两次故障的间隔时间，也就是系统正常运转的平均时间。这个时间越长，系统稳定性越高。</p>
<p><strong>MTTR（Mean Time To Repair）</strong>表示故障的平均恢复时间，也可以理解为平均故障时间。这个值越小，故障对于用户的影响越小。</p>
<p>可用性与 MTBF 和 MTTR 的值息息相关，我们可以用下面的公式表示它们之间的关系：</p>
<p>Availability &#x3D; MTBF &#x2F; (MTBF + MTTR)</p>
<p>这个公式计算出的结果是一个比例，而这个比例代表着系统的可用性。一般来说，我们会使用几个九来描述系统的可用性。</p>
 <img src="/blog/images/pasted-31.png" width = "100%" height = "100%" align=center />

<p>其实通过这张图你可以发现，一个九和两个九的可用性是很容易达到的，只要没有蓝翔技校的铲车搞破坏，基本上可以通过人肉运维的方式实现。</p>
<p>三个九之后，系统的年故障时间从 3 天锐减到 8 小时。到了四个九之后，年故障时间缩减到 1 小时之内。在这个级别的可用性下，你可能需要建立完善的运维值班体系、故障处理流程和业务变更流程。你可能还需要在系统设计上有更多的考虑。比如，在开发中你要考虑，如果发生故障，是否不用人工介入就能自动恢复。当然了，在工具建设方面，你也需要多加完善，以便快速排查故障原因，让系统快速恢复。</p>
<p>到达五个九之后，故障就不能靠人力恢复了。想象一下，从故障发生到你接收报警，再到你打开电脑登录服务器处理问题，时间可能早就过了十分钟了。所以这个级别的可用性考察的是系统的容灾和自动恢复的能力，让机器来处理故障，才会让可用性指标提升一个档次。</p>
<p>一般来说，我们的核心业务系统的可用性，需要达到四个九，非核心系统的可用性最多容忍到三个九。在实际工作中，你可能听到过类似的说法，只是不同级别，不同业务场景的系统对于可用性要求是不一样的。</p>
<p>目前，你已经对可用性的评估指标有了一定程度的了解了，接下来，我们来看一看高可用的系统设计需要考虑哪些因素。</p>
<p>高可用系统设计的思路</p>
<p>一个成熟系统的可用性需要从系统设计和系统运维两方面来做保障，两者共同作用，缺一不可。那么如何从这两方面入手，解决系统高可用的问题呢？</p>
<h3 id="一-系统设计"><a href="#一-系统设计" class="headerlink" title="一.系统设计"></a><a href="#%E4%B8%80-%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1" title="一.系统设计"></a>一.系统设计</h3><hr>
<p>**Design for failure ** 是我们做高可用系统设计时秉持的第一原则。在承担百万 QPS 的高并发系统中，集群中机器的数量成百上千台，单机的故障是常态，几乎每一天都有发生故障的可能。</p>
<p>未雨绸缪才能决胜千里。我们在做系统设计的时候，要把发生故障作为一个重要的考虑点，预先考虑如何自动化地发现故障，发生故障之后要如何解决。当然了，除了要有未雨绸缪的思维之外，我们还需要掌握一些具体的优化方法，比如 <strong>failover（故障转移）、超时控制以及降级和限流。</strong></p>
<h4 id="failover（故障转移）"><a href="#failover（故障转移）" class="headerlink" title="failover（故障转移）"></a><a href="#failover%EF%BC%88%E6%95%85%E9%9A%9C%E8%BD%AC%E7%A7%BB%EF%BC%89" title="failover（故障转移）"></a>failover（故障转移）</h4><p>一般来说，发生 failover 的节点可能有两种情况：</p>
<ol>
<li>是在 完全对等 的节点之间做 failover。</li>
<li>是在 不对等 的节点之间，即系统中存在主节点也存在备节点。</li>
</ol>
<p>在对等节点之间做 failover 相对来说简单些。在这类系统中所有节点都承担读写流量，并且节点中不保存状态，每个节点都可以作为另一个节点的镜像。在这种情况下，如果访问某一个节点失败，那么简单地随机访问另一个节点就好了。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">举个例子，Nginx 可以配置当某一个 Tomcat 出现大于 500 的请求的时候，重试请求另一个 Tomcat 节点，就像下面这样：</span><br></pre></td></tr></table></figure>


 <img src="/blog/images/pasted-32.png" width = "100%" height = "100%" align=center />

<p>针对不对等节点的 failover 机制会复杂很多。比方说我们有一个主节点，有多台备用节点，这些备用节点可以是热备（同样在线提供服务的备用节点），也可以是冷备（只作为备份使用），那么我们就需要在代码中控制如何检测主备机器是否故障，以及如何做主备切换。</p>
<p>使用最广泛的故障检测机制是「心跳」。你可以在客户端上定期地向主节点发送心跳包，也可以从备份节点上定期发送心跳包。当一段时间内未收到心跳包，就可以认为主节点已经发生故障，可以触发选主的操作。</p>
<p>选主的结果需要在多个备份节点上达成一致，所以会使用某一种分布式一致性算法，比方说 Paxos，Raft。</p>
<h4 id="调用超时控制"><a href="#调用超时控制" class="headerlink" title="调用超时控制"></a><a href="#%E8%B0%83%E7%94%A8%E8%B6%85%E6%97%B6%E6%8E%A7%E5%88%B6" title="调用超时控制"></a>调用超时控制</h4><p>除了故障转移以外，对于系统间调用超时的控制也是高可用系统设计的一个重要考虑方面。</p>
<p>复杂的高并发系统通常会有很多的系统模块组成，同时也会依赖很多的组件和服务，比如说缓存组件，队列服务等等。**它们之间的调用最怕的就是延迟而非失败 ** ，因为失败通常是瞬时的，可以通过重试的方式解决。而一旦调用某一个模块或者服务发生比较大的延迟，调用方就会阻塞在这次调用上，它已经占用的资源得不到释放。当存在大量这种阻塞请求时，调用方就会因为用尽资源而挂掉。</p>
<p>在系统开发的初期，超时控制通常不被重视，或者是没有方式来确定正确的超时时间。</p>
<p>我之前经历过一个项目， 模块之间通过 RPC 框架来调用，超时时间是默认的 30 秒。平时系统运行得非常稳定，可是一旦遇到比较大的流量，RPC 服务端出现一定数量慢请求的时候，RPC 客户端线程就会大量阻塞在这些慢请求上长达 30 秒，造成 RPC 客户端用尽调用线程而挂掉。后面我们在故障复盘的时候发现这个问题后，调整了 RPC，数据库，缓存以及调用第三方服务的超时时间，这样在出现慢请求的时候可以触发超时，就不会造成整体系统雪崩。</p>
<p>既然要做超时控制，那么我们怎么来确定超时时间呢？这是一个比较困难的问题。</p>
<p>超时时间短了，会造成大量的超时错误，对用户体验产生影响；超时时间长了，又起不到作用。 我建议你通过收集系统之间的调用日志，统计比如说 99% 的响应时间是怎样的，然后依据这个时间来指定超时时间。 如果没有调用的日志，那么你只能按照经验值来指定超时时间。不过，无论你使用哪种方式，超时时间都不是一成不变的，需要在后面的系统维护过程中不断地修改。</p>
<p>超时控制实际上就是不让请求一直保持，而是在经过一定时间之后让请求失败，释放资源给接下来的请求使用。这对于用户来说是有损的，但是却是必要的，因为它牺牲了少量的请求却保证了整体系统的可用性。而我们还有另外两种有损的方案能保证系统的高可用，它们就是降级和限流。</p>
<h4 id="限流完全是另外一种思路，它通过对并发的请求进行限速来保护系统。"><a href="#限流完全是另外一种思路，它通过对并发的请求进行限速来保护系统。" class="headerlink" title="限流完全是另外一种思路，它通过对并发的请求进行限速来保护系统。"></a><a href="#%E9%99%90%E6%B5%81%E5%AE%8C%E5%85%A8%E6%98%AF%E5%8F%A6%E5%A4%96%E4%B8%80%E7%A7%8D%E6%80%9D%E8%B7%AF%EF%BC%8C%E5%AE%83%E9%80%9A%E8%BF%87%E5%AF%B9%E5%B9%B6%E5%8F%91%E7%9A%84%E8%AF%B7%E6%B1%82%E8%BF%9B%E8%A1%8C%E9%99%90%E9%80%9F%E6%9D%A5%E4%BF%9D%E6%8A%A4%E7%B3%BB%E7%BB%9F%E3%80%82" title="限流完全是另外一种思路，它通过对并发的请求进行限速来保护系统。"></a>限流完全是另外一种思路，它通过对并发的请求进行限速来保护系统。</h4><p>比如对于 Web 应用，我限制单机只能处理每秒 1000 次的请求，超过的部分直接返回错误给客户端。虽然这种做法损害了用户的使用体验，但是它是在极端并发下的无奈之举，是短暂的行为，因此是可以接受的。</p>
<h4 id="降级"><a href="#降级" class="headerlink" title="降级"></a><a href="#%E9%99%8D%E7%BA%A7" title="降级"></a>降级</h4><p>降级是为了保证核心服务的稳定而牺牲非核心服务的做法。 比方说我们发一条微博会先经过反垃圾服务检测，检测内容是否是广告，通过后才会完成诸如写数据库等逻辑。</p>
<p>反垃圾的检测是一个相对比较重的操作，因为涉及到非常多的策略匹配，在日常流量下虽然会比较耗时却还能正常响应。但是当并发较高的情况下，它就有可能成为瓶颈，而且它也不是发布微博的主体流程，所以我们可以暂时关闭反垃圾服务检测，这样就可以保证主体的流程更加稳定。</p>
<h3 id="二-系统运维"><a href="#二-系统运维" class="headerlink" title="二. 系统运维"></a><a href="#%E4%BA%8C-%E7%B3%BB%E7%BB%9F%E8%BF%90%E7%BB%B4" title="二. 系统运维"></a>二. 系统运维</h3><hr>
<p>在系统设计阶段为了保证系统的可用性可以采取上面的几种方法，那在系统运维的层面又能做哪些事情呢？其实，我们可以从 灰度发布、故障演练 两个方面来考虑如何提升系统的可用性。</p>
<p>你应该知道，在业务平稳运行过程中，系统是很少发生故障的，90% 的故障是发生在上线变更阶段的。比方说，你上了一个新的功能，由于设计方案的问题，数据库的慢请求数翻了一倍，导致系统请求被拖慢而产生故障。</p>
<p>如果没有变更，数据库怎么会无缘无故地产生那么多的慢请求呢？因此，为了提升系统的可用性，重视变更管理尤为重要。而除了提供必要回滚方案，以便在出现问题时快速回滚恢复之外， 另一个主要的手段就是灰度发布。</p>
<h4 id="灰度发布"><a href="#灰度发布" class="headerlink" title="灰度发布"></a><a href="#%E7%81%B0%E5%BA%A6%E5%8F%91%E5%B8%83" title="灰度发布"></a>灰度发布</h4><p>灰度发布指的是系统的变更不是一次性地推到线上的，而是按照一定比例逐步推进的。一般情况下，灰度发布是以机器维度进行的。比方说，我们先在 10% 的机器上进行变更，同时观察 Dashboard 上的系统性能指标以及错误日志。如果运行了一段时间之后系统指标比较平稳并且没有出现大量的错误日志，那么再推动全量变更。</p>
<p>灰度发布给了开发和运维同学绝佳的机会，让他们能在线上流量上观察变更带来的影响，是保证系统高可用的重要关卡。</p>
<p>灰度发布是在系统正常运行条件下，保证系统高可用的运维手段，那么我们如何知道发生故障时系统的表现呢？这里就要依靠另外一个手段： 故障演练。</p>
<h4 id="故障演练"><a href="#故障演练" class="headerlink" title="故障演练"></a><a href="#%E6%95%85%E9%9A%9C%E6%BC%94%E7%BB%83" title="故障演练"></a>故障演练</h4><p>故障演练指的是对系统进行一些破坏性的手段，观察在出现局部故障时，整体的系统表现是怎样的，从而发现系统中存在的，潜在的可用性问题。</p>
<p>一个复杂的高并发系统依赖了太多的组件，比方说磁盘，数据库，网卡等，这些组件随时随地都可能会发生故障，而一旦它们发生故障，会不会如蝴蝶效应一般造成整体服务不可用呢？我们并不知道，因此，故障演练尤为重要。</p>
<p>在我来看， 故障演练和时下比较流行的“混沌工程”的思路如出一辙， 作为混沌工程的鼻祖，Netfix 在 2010 年推出的 Chaos Monkey 工具就是故障演练绝佳的工具。它通过在线上系统上随机地关闭线上节点来模拟故障，让工程师可以了解，在出现此类故障时会有什么样的影响。</p>
<p>当然，这一切是以你的系统可以抵御一些异常情况为前提的。如果你的系统还没有做到这一点，那么 我建议你 另外搭建一套和线上部署结构一模一样的线下系统，然后在这套系统上做故障演练，从而避免对生产系统造成影响。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://irisalex.github.io/blog/2023/11/29/%E7%B3%BB%E7%BB%9F%E9%AB%98%E5%8F%AF%E7%94%A8/" data-id="cm879ko98000y3zjj1ibphtcv" data-title="系统高可用怎么做" class="article-share-link">Share</a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-系统设计目标" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/blog/2023/11/29/%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1%E7%9B%AE%E6%A0%87/" class="article-date">
  <time class="dt-published" datetime="2023-11-29T11:21:16.000Z" itemprop="datePublished">2023-11-29</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/blog/2023/11/29/%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1%E7%9B%AE%E6%A0%87/">系统设计目标</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="如何提升系统性能"><a href="#如何提升系统性能" class="headerlink" title="如何提升系统性能"></a><a href="#%E5%A6%82%E4%BD%95%E6%8F%90%E5%8D%87%E7%B3%BB%E7%BB%9F%E6%80%A7%E8%83%BD" title="如何提升系统性能"></a>如何提升系统性能</h1><p>提到互联网系统设计，你可能听到最多的词儿就是 三高，也就是 高并发、高性能、高可用，它们是互联网系统架构设计永恒的主题。在前两节课中，我带你了解了高并发系统设计的含义，意义以及分层设计原则，接下来，我想带你整体了解一下高并发系统设计的目标，然后在此基础上，进入我们今天的话题：如何提升系统的性能？</p>
<h3 id="高并发系统设计的三大目标"><a href="#高并发系统设计的三大目标" class="headerlink" title="高并发系统设计的三大目标"></a><a href="#%E9%AB%98%E5%B9%B6%E5%8F%91%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1%E7%9A%84%E4%B8%89%E5%A4%A7%E7%9B%AE%E6%A0%87" title="高并发系统设计的三大目标"></a>高并发系统设计的三大目标</h3><hr>
<p>高并发， 是指运用设计手段让系统能够处理更多的用户并发请求，也就是承担更大的流量。它是一切架构设计的背景和前提，脱离了它去谈性能和可用性是没有意义的。很显然嘛，你在每秒一次请求和每秒一万次请求，两种不同的场景下，分别做到毫秒级响应时间和五个九（99.999%）的可用性，无论是设计难度还是方案的复杂度，都不是一个级别的。</p>
<p>而性能和可用性， 是我们实现高并发系统设计必须考虑的因素。</p>
<p>性能反应了系统的使用体验，想象一下，同样承担每秒一万次请求的两个系统，一个响应时间是毫秒级，一个响应时间在秒级别，它们带给用户的体验肯定是不同的。</p>
<p>可用性则表示系统可以正常服务用户的时间。我们再类比一下，还是两个承担每秒一万次的系统，一个可以做到全年不停机、无故障，一个隔三差五宕机维护，如果你是用户，你会选择使用哪一个系统呢？答案不言而喻。</p>
<p>另一个耳熟能详的名词叫 可扩展性 ，它同样是高并发系统设计需要考虑的因素。为什么呢？我来举一个具体的例子。</p>
<p>流量分为 平时流量 和 峰值流量 两种，峰值流量可能会是平时流量的几倍甚至几十倍，在应对峰值流量的时候，我们通常需要在架构和方案上做更多的准备。这就是淘宝会花费大半年的时间准备双十一，也是在面对「明星离婚」等热点事件时，看起来无懈可击的微博系统还是会出现服务不可用的原因。 而易于扩展的系统能在短时间内迅速完成扩容，更加平稳地承担峰值流量。</p>
<p>高性能、高可用和可扩展，是我们在做高并发系统设计时追求的三个目标，我会用三节课的时间，带你了解在高并发大流量下如何设计高性能、高可用和易于扩展的系统。</p>
<p>了解完这些内容之后，我们正式进入今天的话题：如何提升系统的性能？</p>
<h3 id="性能优化原则"><a href="#性能优化原则" class="headerlink" title="性能优化原则"></a><a href="#%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E5%8E%9F%E5%88%99" title="性能优化原则"></a>性能优化原则</h3><hr>
<p>「天下武功，唯快不破」。性能是系统设计成功与否的关键，实现高性能也是对程序员个人能力的挑战。不过在了解实现高性能的方法之前，我们先明确一下性能优化的原则。</p>
<ul>
<li><p>首先，性能优化一定不能盲目，一定是问题导向的</p>
<p>脱离了问题，盲目地提早优化会增加系统的复杂度，浪费开发人员的时间，也因为某些优化可能会对业务上有些折中的考虑，所以也会损伤业务。</p>
</li>
<li><p>其次，性能优化也遵循「八二原则」</p>
<p>即你可以用 20% 的精力解决 80% 的性能问题。所以我们在优化过程中一定要抓住主要矛盾，优先优化主要的性能瓶颈点。</p>
</li>
<li><p>再次，性能优化也要有数据支撑</p>
<p>在优化过程中，你要时刻了解你的优化让响应时间减少了多少，提升了多少的吞吐量。</p>
</li>
<li><p>最后，性能优化的过程是持续的</p>
<p>高并发的系统通常是业务逻辑相对复杂的系统，那么在这类系统中出现的性能问题通常也会有多方面的原因。因此，我们在做性能优化的时候要明确目标，比方说，支撑每秒 1 万次请求的吞吐量下响应时间在 10ms，那么我们就需要持续不断地寻找性能瓶颈，制定优化方案，直到达到目标为止。</p>
</li>
</ul>
<p>在以上四个原则的指引下，掌握常见性能问题的排查方式和优化手段，就一定能让你在设计高并发系统时更加游刃有余。</p>
<h3 id="性能的度量指标"><a href="#性能的度量指标" class="headerlink" title="性能的度量指标"></a><a href="#%E6%80%A7%E8%83%BD%E7%9A%84%E5%BA%A6%E9%87%8F%E6%8C%87%E6%A0%87" title="性能的度量指标"></a>性能的度量指标</h3><hr>
<p>对于性能我们需要有度量的标准，有了数据才能明确目前存在的性能问题，也能够用数据来评估性能优化的效果。 所以明确性能的度量指标十分重要。</p>
<p>一般来说，度量性能的指标是 系统接口的响应时间，但是单次的响应时间是没有意义的，你需要知道一段时间的性能情况是什么样的。所以，我们需要收集这段时间的响应时间数据，然后依据一些统计方法计算出 特征值，这些特征值就能够代表这段时间的性能情况。我们常见的特征值有以下几类。</p>
<ul>
<li><p><strong>平均值</strong></p>
<p>顾名思义，平均值是把这段时间所有请求的响应时间数据相加，再除以总请求数。平均值可以在一定程度上反应这段时间的性能，但它敏感度比较差，如果这段时间有少量慢请求时，在平均值上并不能如实的反应。</p>
<p>举个例子，假设我们在 30s 内有 10000 次请求，每次请求的响应时间都是 1ms，那么这段时间响应时间平均值也是 1ms。这时，当其中 100 次请求的响应时间变成了 100ms，那么整体的响应时间是 100 * 100 + 9900 * 1) &#x2F; 10000 &#x3D; 1.99ms。你看，虽然从平均值上来看仅仅增加了不到 1ms，但是实际情况是有 1% 的请求（100&#x2F;10000） 的响应时间已经增加了 100 倍。 所以，平均值对于度量性能来说只能作为一个参考。</p>
</li>
<li><p><strong>最大值</strong></p>
<p>这个更好理解，就是这段时间内所有请求响应时间最长的值，但它的问题又在于过于敏感了。</p>
<p>还拿上面的例子来说，如果 10000 次请求中只有一次请求的响应时间达到 100ms，那么这段时间请求的响应耗时的最大值就是 100ms，性能损耗为原先的百分之一，这种说法明显是不准确的。</p>
</li>
<li><p><strong>分位值</strong></p>
<p>分位值有很多种，比如 90 分位、95 分位、75 分位。以 90 分位为例，我们把这段时间请求的 响应时间从小到大排序，假如一共有 100 个请求，那么排在第 90 位的响应时间就是 90 分位值。分位值排除了偶发极慢请求对于数据的影响，能够很好地反应这段时间的性能情况，分位值越大，对于慢请求的影响就越敏感。</p>
</li>
</ul>
 <img src="/blog/images/pasted-29.png" width = "100%" height = "100%" align=center />

<p>分位值是最适合作为时间段内，响应时间统计值来使用的，在实际工作中也应用最多。除此之外，平均值也可以作为一个参考值来使用。</p>
<p>我在上面提到，脱离了并发来谈性能是没有意义的，我们通常使用 吞吐量 或者 同时在线用户数 来度量并发和流量，使用吞吐量的情况会更多一些。但是你要知道，这两个指标是呈倒数关系的。</p>
<p>这很好理解，响应时间 1s 时，吞吐量是每秒 1 次，响应时间缩短到 10ms，那么吞吐量就上升到每秒 100 次。所以，一般我们度量性能时都会同时兼顾吞吐量和响应时间，比如我们设立性能优化的目标时通常会这样表述：在每秒 1 万次的请求量下，响应时间 99 分位值在 10ms 以下。</p>
<p>那么，响应时间究竟控制在多长时间比较合适呢？这个不能一概而论。</p>
<p><strong>用户角度看</strong></p>
<ul>
<li><p>200ms 是第一个分界点</p>
<p>接口的响应时间在 200ms 之内，用户是感觉不到延迟的，就像是瞬时发生的一样。</p>
</li>
<li><p>而 1s 是另外一个分界点</p>
<p>接口的响应时间在 1s 之内时，虽然用户可以感受到一些延迟，但却是可以接受的</p>
</li>
<li><p>超过 1s 之后用户就会有明显等待的感觉，等待时间越长，用户的使用体验就越差。</p>
<p>所以，健康系统的 99 分位值的响应时间通常需要控制在 200ms 之内，而不超过 1s 的请求占比要在 99.99% 以上。</p>
</li>
</ul>
<p>现在了解了性能的度量指标，那我们再来看一看，随着并发的增长我们实现高性能的思路是怎样的</p>
<h3 id="高并发下的性能优化"><a href="#高并发下的性能优化" class="headerlink" title="高并发下的性能优化"></a><a href="#%E9%AB%98%E5%B9%B6%E5%8F%91%E4%B8%8B%E7%9A%84%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96" title="高并发下的性能优化"></a>高并发下的性能优化</h3><hr>
<p>假如说，你现在有一个系统，这个系统中处理核心只有一个，执行的任务的响应时间都在 10ms，它的吞吐量是在每秒 100 次。那么我们如何来优化性能从而提高系统的并发能力呢？主要有两种思路：</p>
<ol>
<li>是提高系统的处理核心数</li>
<li>是减少单次任务的响应时间。</li>
</ol>
<p><strong>一. 提高系统的处理核心数</strong></p>
<p>提高系统的处理核心数就是 增加系统的并行处理能力，这个思路是优化性能最简单的途径。拿上一个例子来说，你可以把系统的处理核心数增加为两个，并且增加一个进程，让这两个进程跑在不同的核心上。这样从理论上，你系统的吞吐量可以增加一倍。当然了，在这种情况下，吞吐量和响应时间就不是倒数关系了，而是：吞吐量 &#x3D; 并发进程数 &#x2F; 响应时间。</p>
<p>计算机领域的阿姆达尔定律（Amdahl’s law）是吉恩·阿姆达尔在 1967 年提出的。它描述了并发进程数与响应时间之间的关系，含义是在固定负载下，并行计算的加速比，也就是并行化之后效率提升情况，可以用下面公式来表示：(Ws + Wp) &#x2F; (Ws + Wp&#x2F;s)</p>
<p>其中，Ws 表示任务中的串行计算量，Wp 表示任务中的并行计算量，s 表示并行进程数。从这个公式我们可以推导出另外一个公式：1&#x2F;(1-p+p&#x2F;s)</p>
<p>其中，s 还是表示并行进程数，p 表示任务中并行部分的占比。当 p 为 1 时，也就是完全并行时，加速比与并行进程数相等；当 p 为 0 时，即完全串行时，加速比为 1，也就是说完全无加速；当 s 趋近于无穷大的时候，加速比就等于 1&#x2F;(1-p)，你可以看到它完全和 p 成正比。特别是，当 p 为 1 时，加速比趋近于无穷大。</p>
<p>以上公式的推导过程有些复杂，你只需要记住结论就好了。</p>
<p>我们似乎找到了解决问题的银弹，<strong>是不是无限制地增加处理核心数就能无限制地提升性能，从而提升系统处理高并发的能力呢？</strong>很遗憾，随着并发进程数的增加，并行的任务对于系统资源的争抢也会愈发严重。在某一个临界点上继续增加并发进程数，反而会造成系统性能的下降，这就是性能测试中的 拐点模型。</p>
 <img src="/blog/images/pasted-30.png" width = "100%" height = "100%" align=center />

<p>从图中可以发现：</p>
<ol>
<li><p>并发用户数处于轻压力区时，响应时间平稳，吞吐量和并发用户数线性相关。</p>
</li>
<li><p>而当并发用户数处于重压力区时，系统资源利用率到达极限，吞吐量开始有下降的趋势，响应时间也会略有上升。</p>
</li>
<li><p>这个时候，再对系统增加压力，系统就进入拐点区，处于超负荷状态，吞吐量下降，响应时间大幅度上升。</p>
</li>
</ol>
<p>所以我们在评估系统性能时通常需要做压力测试，目的就是找到系统的「拐点」，从而知道系统的承载能力，也便于找到系统的瓶颈，持续优化系统性能。</p>
<p>说完了提升并行能力，我们再看看优化性能的另一种方式：减少单次任务响应时间。</p>
<p><strong>二. 减少单次任务响应时间</strong></p>
<p>想要减少任务的响应时间，首先要看你的系统是 CPU 密集型 还是 IO 密集型 的，因为不同类型的系统性能优化方式不尽相同。</p>
<ol>
<li><p>CPU 密集型系统</p>
<p>CPU 密集型系统中，需要处理大量的 CPU 运算，那么选用更高效的算法或者减少运算次数就是这类系统重要的优化手段。比方说，如果系统的主要任务是计算 Hash 值，那么这时选用更高性能的 Hash 算法就可以大大提升系统的性能。发现这类问题的主要方式，是通过一些 Profile 工具来找到消耗 CPU 时间最多的方法或者模块，比如 Linux 的 perf、eBPF 等</p>
</li>
<li><p>IO 密集型系统</p>
<p>IO 密集型系统指的是系统的大部分操作是在等待 IO 完成，这里 IO 指的是磁盘 IO 和网络 IO。我们熟知的系统大部分都属于 IO 密集型，比如数据库系统、缓存系统、Web 系统。这类系统的性能瓶颈可能出在系统内部，也可能是依赖的其他系统，而发现这类性能瓶颈的手段主要有两类。</p>
<p>2.1 第一类是 采用工具</p>
<p>Linux 的工具集很丰富，完全可以满足你的优化需要，比如网络协议栈、网卡、磁盘、文件系统、内存，等等。这些工具的用法很多，你可以在排查问题的过程中逐渐积累。除此之外呢，一些开发语言还有针对语言特性的分析工具，比如说 go 语言就有其专属的内存分析工具。</p>
<p>2.2 另外一类手段就是可以通过 监控 来发现性能问题。</p>
<p>在监控中我们可以对任务的每一个步骤做分时的统计，从而找到任务的哪一步消耗了更多的时间。这一部分在演进篇中会有专门的介绍，这里就不再展开了。</p>
<p>那么找到了系统的瓶颈点，我们要如何优化呢？优化方案会随着问题的不同而不同。比方说，如果是数据库访问慢，那么就要看是不是有锁表的情况、是不是有全表扫描、索引加得是否合适、是否有 JOIN 操作、需不需要加缓存，等等；如果是网络的问题，就要看网络的参数是否有优化的空间，抓包来看是否有大量的超时重传，网卡是否有大量丢包等。</p>
</li>
</ol>
<h4 id="小结"><a href="#小结" class="headerlink" title="小结"></a><a href="#%E5%B0%8F%E7%BB%93" title="小结"></a>小结</h4><p>了解了性能的原则、度量指标，以及在高并发下优化性能的基本思路。性能优化是一个很大的话题，上面所讲是完全不够的，比方说我们可以用缓存优化系统的读取性能，使用消息队列优化系统的写入性能等等。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://irisalex.github.io/blog/2023/11/29/%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1%E7%9B%AE%E6%A0%87/" data-id="cm879ko97000x3zjj9jyycxsu" data-title="系统设计目标" class="article-share-link">Share</a>
      
      
      
    </footer>
  </div>
  
</article>



  


  <nav id="page-nav">
    
    <span class="page-number current">1</span><a class="page-number" href="/blog/page/2/">2</a><a class="extend next" rel="next" href="/blog/page/2/">Next &raquo;</a>
  </nav>
</section>
        <aside id="sidebar">
  
    

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/blog/2025/03/13/Ai/">大模型</a>
          </li>
        
          <li>
            <a href="/blog/2023/12/04/8%E7%A7%8D%E6%B5%81%E8%A1%8C%E7%9A%84%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B/">8种流行的网络模型</a>
          </li>
        
          <li>
            <a href="/blog/2023/12/01/NoSQL-%E7%9A%84%E5%BA%94%E7%94%A8/">NoSQL 的应用</a>
          </li>
        
          <li>
            <a href="/blog/2023/12/01/%E6%95%B0%E6%8D%AE%E7%9A%84%E5%94%AF%E4%B8%80%E6%80%A7/">数据的唯一性</a>
          </li>
        
          <li>
            <a href="/blog/2023/12/01/%E6%95%B0%E6%8D%AE%E4%BC%98%E5%8C%96%E6%96%B9%E6%A1%882/">数据优化方案2</a>
          </li>
        
      </ul>
    </div>
  </div>

  
    

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/blog/archives/2025/03/">March 2025</a></li><li class="archive-list-item"><a class="archive-list-link" href="/blog/archives/2023/12/">December 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/blog/archives/2023/11/">November 2023</a></li></ul>
    </div>
  </div>

  
</aside>
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2025 By Autoload<br>
      Driven - <a href="https://hexo.io/" target="_blank">Hexo</a>|Theme - <a href="https://github.com/autoload/hexo-theme-auto" target="_blank">Auto</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/blog/" class="mobile-nav-link">Home</a>
  
    <a href="/blog/archives" class="mobile-nav-link">Archives</a>
  
    <a href="/blog/categories" class="mobile-nav-link">Categories</a>
  
    <a href="/blog/tags" class="mobile-nav-link">Tags</a>
  
    <a href="/blog/about" class="mobile-nav-link">About</a>
  
</nav>
    


<script src="/blog/js/jquery-3.4.1.min.js"></script>


<script src="/blog/js/script.js"></script>




  </div>
</body>
</html>